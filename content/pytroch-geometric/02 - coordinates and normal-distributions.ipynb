{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe4a48d-b92d-4398-8702-b72be6c8f7e3",
   "metadata": {},
   "source": [
    "### Imports and HTML-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e013a98-618b-4936-9dd9-dbca51355186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918d74a9-7889-4a00-8727-6a148c65606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bf5323-ff7a-4905-bda6-15f9f7b16d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "point_to_dist_angle = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fcb20c-672e-4ea1-a387-442a16fa7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def calc_likelihoods_for_distributions_and_points(matrix_dist, matrix_points):\n",
    "    means = matrix_dist[:, 0]\n",
    "    covariances = matrix_dist[:, 1:]\n",
    "    likelihoods = np.array([multivariate_normal(mean=means[i], cov=covariances[i]).pdf(matrix_points) for i in range(len(means))])\n",
    "    \n",
    "    return likelihoods.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5536ff7-9c89-4a04-9600-7f8cb557cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def plot_distributions_with_ids_with_colored_points(distributions, ids, points, points_to_distribution, n_std=1.96):\n",
    "    plt, ax = plot_confidence_ellipses_with_ids(distributions, ids, n_std)\n",
    "\n",
    "    for dist_idx, point_idx in enumerate(points_to_distribution):\n",
    "        if point_idx == -1: \n",
    "            continue\n",
    "        color = generate_color_for_idx(ids[dist_idx])\n",
    "        ax.plot(points[point_idx, 0], points[point_idx, 1], 'o', color=color)\n",
    "\n",
    "    return plt, ax\n",
    "\n",
    "def end_to_end_visualize_graphs_by_distributions_and_points(graph_a, graph_b):\n",
    "    node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "    \n",
    "    distributions_a = get_distribution_from_graph(graph_a)\n",
    "    ids_a = [int(x) for x in node_attributes_a[3].tolist()]\n",
    "    points_b = node_attributes_b[2]\n",
    "\n",
    "    points_to_distributions = calc_point_indices_to_distributions(distributions_a, points_b, 0)\n",
    "    \n",
    "    return plot_distributions_with_ids_with_colored_points(distributions_a, ids_a, points_b, points_to_distributions, n_std=1.96)\n",
    "\n",
    "def rotate_plot(fig, angle):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf)\n",
    "    \n",
    "    rotated_image = image.rotate(angle, expand=True, resample=Image.BICUBIC)\n",
    "    \n",
    "    return rotated_image\n",
    "\n",
    "def create_and_play_html_video(images_cache, size=480, dpi=600, interpolation='bicubic'):\n",
    "    def update(i):\n",
    "        ax.clear()\n",
    "        ax.imshow(images_cache[i], interpolation=interpolation)\n",
    "        ax.axis('off')\n",
    "\n",
    "    inches_per_pixel = 1 / dpi\n",
    "    fig_size_in_inches = (size * inches_per_pixel, size * inches_per_pixel)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=fig_size_in_inches, dpi=dpi)\n",
    "    \n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "    \n",
    "    ani = FuncAnimation(fig, update, frames=len(images_cache), interval=50)\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492b55e0-3feb-4384-9b4c-e0ee2c836355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "filter_max_label_number = 1\n",
    "\n",
    "# csv-header: iteration, class, top, left, bottom, right, center_x, center_y, object_id\n",
    "def csv_entry_for_graph(graph, relative_path='./csv_files'):\n",
    "    global filter_max_label_number\n",
    "    \n",
    "    classes_dict = {\n",
    "        0: 'goomba',\n",
    "        1: 'mario',\n",
    "        2: 'cloud',\n",
    "        3: 'ground',\n",
    "        4: 'bush',\n",
    "        5: 'box',\n",
    "        6: 'pipe'\n",
    "    }\n",
    "\n",
    "    for i in range(len(graph.x)):\n",
    "        graph_node = graph.x[i][6:]\n",
    "        iteration = int(graph_node[6]) # iteration will start at 1\n",
    "        class_num = int(graph_node[0])\n",
    "        class_name = classes_dict.get(class_num, 'unknown')\n",
    "        width = round(float(graph_node[3]), 2)\n",
    "        height = round(float(graph_node[4]), 2)\n",
    "        center_x = round(float(graph_node[1]), 2)\n",
    "        center_y = round(float(graph_node[2]), 2)\n",
    "        top = round(center_y - (height / 2), 2)\n",
    "        left = round(center_x - (width / 2), 2)\n",
    "        bottom = round(center_y + (height / 2), 2)\n",
    "        right = round(center_x + (width / 2), 2)\n",
    "        object_id = int(graph_node[7])\n",
    "    \n",
    "        file_number = int(graph_node[5])  # write to csv-file with name {file_number:04d}.csv\"\n",
    "        \n",
    "        csv_entry = [iteration, class_name, top, left, bottom, right, center_x, center_y, object_id]\n",
    "        file_name = os.path.join(relative_path, f\"{file_number:04d}.csv\")\n",
    "        \n",
    "        if iteration == 1 and os.path.isfile(file_name):\n",
    "            os.remove(file_name)\n",
    "    \n",
    "        with open(file_name, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            if iteration == 1:\n",
    "                writer.writerow(['iteration', 'class', 'top', 'left', 'bottom', 'right', 'center_x', 'center_y', 'object_id'])\n",
    "                filter_max_label_number = 1\n",
    "            \n",
    "            writer.writerow(csv_entry)\n",
    "\n",
    "@save_params\n",
    "def graph_id_tracking(graph_a, graph_b, treshhold=0):\n",
    "    global filter_max_label_number\n",
    "    mappings = find_mapping_of_two_graphs(graph_a, graph_b, treshhold)\n",
    "    \n",
    "    all_node_ids = torch.tensor([int(node_entry[-1]) for node_entry in graph_b.x])\n",
    "    not_tracked_nodes = all_node_ids[~all_node_ids.unsqueeze(-1).eq( mappings[:, 1]).any(dim=-1)]\n",
    "\n",
    "    indicies_graph_b = graph_b.x[:, -1]\n",
    "    indicies_graph_a = graph_a.x[:, -1]\n",
    "    max_graph_value = max(max(indicies_graph_a), max(indicies_graph_b))\n",
    "\n",
    "    # if (graph_b.x[0, -2] > 127): sys.exit(-1)\n",
    "    for i, object_id in enumerate(indicies_graph_b):\n",
    "        if(object_id in not_tracked_nodes):\n",
    "            graph_b.x[i, -1] = filter_max_label_number\n",
    "            # print('filter_max_label_number', filter_max_label_number)\n",
    "            filter_max_label_number += 1\n",
    "        else:\n",
    "            # mapping_idx = int(torch.where(mappings[:, 1] == int(object_id))[0]) \n",
    "            mapping_idx_tensor = torch.where(mappings[:, 1] == object_id)[0]\n",
    "            mapping_idx = mapping_idx_tensor.item()\n",
    "            graph_b.x[i, -1] = mappings[mapping_idx, 0]\n",
    "            # print('mappings', mappings[mapping_idx, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d1e96c6-f5f7-4afc-bce6-eecef718fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(csv_file_name, iteration):\n",
    "    classes_dict = {\n",
    "        'goomba': 0,\n",
    "        'mario': 1,\n",
    "        'cloud': 2,\n",
    "        'ground': 3,\n",
    "        'bush': 4,\n",
    "        'box': 5,\n",
    "        'pipe': 6\n",
    "    }\n",
    "    \n",
    "    class_names, boxes = get_classnames_boxes_from_csv(csv_file_name, iteration)\n",
    "    num_nodes = len(boxes)\n",
    "    edge_connections = cartesian_product_for_nodes(range(num_nodes))\n",
    "    node_features = []\n",
    "    matrix = np.empty((0, 2))\n",
    "    normal_dist = [0, 0, 10, 0, 0, 10] # mu1, mu2, sig00, sig01, sig10, sig11\n",
    "    dataset_number = int(csv_file_name.split('/')[-1].split('.')[0])\n",
    "\n",
    "    if num_nodes == 1:\n",
    "        box = boxes[0]\n",
    "        width, height = abs(box['left'] - box['right']), abs(box['top'] - box['bottom'])\n",
    "        # node_feature: (normal-distribution, class-label, x-val, y-val, width, height, dataset_number, iteration, id)\n",
    "        node_features.append((*normal_dist, classes_dict[class_names[0]], box['center_x'], \\\n",
    "                              box['center_y'], width, height, dataset_number, iteration, 0))\n",
    "        \n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        return Data(\n",
    "            x=node_features,\n",
    "            edge_index=torch.tensor([0]),\n",
    "            edge_attr=torch.tensor([0])\n",
    "        )\n",
    "        \n",
    "    for i, box in enumerate(boxes):\n",
    "        new_row = np.array([[box['center_x'], box['center_y']]])\n",
    "        matrix = np.vstack((matrix, new_row))\n",
    "\n",
    "        width, height = abs(box['left'] - box['right']), abs(box['top'] - box['bottom'])\n",
    "        # node_feature: (normal-distribution, class-label, x-val, y-val, width, height, dataset_number, iteration, id)\n",
    "        node_features.append((*normal_dist, classes_dict[class_names[i]], box['center_x'], \\\n",
    "                              box['center_y'], width, height, dataset_number, iteration, i))\n",
    "    \n",
    "    dists, angles = dist_angle_from_matrix(matrix, edge_connections)\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_connections = torch.tensor(edge_connections)\n",
    "    edges_features =  torch.tensor(np.stack((dists, angles), axis=-1))\n",
    "    \n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_connections.t().contiguous(),\n",
    "        edge_attr=edges_features\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f016742a-f3ea-459f-a59a-7504a1620031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_point_indices_to_distributions(distributions, points, treshhold_likelihood = 0):\n",
    "    if points.shape[0] == 0:\n",
    "        return np.zeros(distributions.shape[0]) - 1\n",
    "    \n",
    "    if points.shape[0] == 1 and distributions.shape[0] == 1:\n",
    "        likelihood_entries = calc_likelihoods_for_distributions_and_points(distributions, points)\n",
    "    \n",
    "        if likelihood_entries < treshhold_likelihood: return np.array([-1])\n",
    "        return np.array([0])\n",
    "\n",
    "    likelihood_entries, likelihood_indicies_to_filter = calc_likelihood_entries_to_distributions(distributions, points, treshhold_likelihood)\n",
    "\n",
    "    return calc_point_indicies_to_likelihoods(likelihood_entries, likelihood_indicies_to_filter)\n",
    "\n",
    "def calc_point_indicies_to_likelihoods_no_filter(likelihood_entries):\n",
    "    cost_matrix = -1 * np.log(likelihood_entries)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    points_to_distributions = np.zeros(likelihood_entries.shape[1], dtype=row_ind.dtype) - 1\n",
    "    points_to_distributions[col_ind] = row_ind\n",
    "\n",
    "    return points_to_distributions\n",
    "\n",
    "def calc_point_indicies_to_likelihoods(likelihood_entries, likelihood_indicies_to_filter):\n",
    "    cost_matrix = -1 * np.log(likelihood_entries)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    points_to_distributions = np.zeros(likelihood_entries.shape[1], dtype=row_ind.dtype) - 1\n",
    "    \n",
    "    points_to_distributions[col_ind] = row_ind\n",
    "\n",
    "    for point_index, distribution_index in enumerate(col_ind):\n",
    "        if likelihood_indicies_to_filter[point_index, distribution_index]:\n",
    "            points_to_distributions[point_index] = -1\n",
    "    \n",
    "    return points_to_distributions\n",
    "\n",
    "def calc_likelihood_entries_to_distributions(distributions, points, treshhold_likelihood=0):\n",
    "    likelihoods = calc_likelihoods_for_distributions_and_points(distributions, points)\n",
    "    sorted_indices = np.argsort(likelihoods, axis=0)\n",
    "    ranks = np.zeros_like(likelihoods, dtype=int)\n",
    "    n_rows, n_cols = likelihoods.shape\n",
    "    ranks[sorted_indices, np.arange(n_cols)] = np.tile(np.arange(n_rows), (n_cols, 1)).T\n",
    "    \n",
    "    mask_binary = np.array((n_rows - ranks) <= n_cols, dtype=int)\n",
    "    cumsum_array = np.cumsum(mask_binary, axis=0)\n",
    "    \n",
    "    s = min(n_rows, n_cols)\n",
    "    extended_likelihood_entries = np.zeros((s+1, n_cols + 1))\n",
    "    \n",
    "    mask  = np.array((n_rows - ranks) <= n_cols)\n",
    "    \n",
    "    points_to_consider = np.where(np.any(mask, axis=1))[0]\n",
    "    filtered_points = points[points_to_consider]\n",
    "    \n",
    "    likelihood_entries = calc_likelihoods_for_distributions_and_points(distributions, filtered_points)\n",
    "    likelihood_indicies_to_filter = likelihood_entries < treshhold_likelihood\n",
    "    # punish points that should be filtered out\n",
    "    likelihood_entries[likelihood_indicies_to_filter] = 1e-50\n",
    "\n",
    "    return likelihood_entries, likelihood_indicies_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0fbe299-afc3-4748-9e05-1bca777e972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_points_distributions_ids_from_graph(graph):\n",
    "    graph = graph.x\n",
    "    return torch.clone(graph[:, 6]), torch.clone(graph[:, :6]), torch.clone(graph[:, 7:9]), torch.clone(graph[:, -1])\n",
    "\n",
    "def indicies_of_filterted_array_entries(filtered_indicies, relative_indicies):\n",
    "    return np.where(filtered_indicies)[0][relative_indicies]\n",
    "\n",
    "def calculate_sse_for_distributions_and_points(graph_a, graph_b, mappings):\n",
    "    node_attr_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attr_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "    \n",
    "    dist_indicies = mappings[:, 0]\n",
    "    distributions = node_attr_a[1][dist_indicies]\n",
    "    distributions[:, :2] +=  np.array(node_attr_a[2][dist_indicies])[:]\n",
    "    distributions = np.array(distributions.reshape(distributions.shape[0], -1, 2))\n",
    "    \n",
    "    points = node_attr_b[2][mappings[:, 1]]\n",
    "    \n",
    "    likelihoods = torch.tensor(calc_likelihoods_for_distributions_and_points(distributions, points))\n",
    "    if likelihoods.shape[0] == 1: return likelihoods\n",
    "    \n",
    "    return likelihoods[mappings[:, 0], mappings[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda9a6a2-bcdb-4295-a378-4d001e265d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_distributions__with_ids_with_colored_points(distributions, ids, points, points_to_distribution, n_std=1.96):\n",
    "#     fig, ax = plot_confidence_ellipses(distributions, n_std)\n",
    "    \n",
    "#     # Plot all points in a default color (black)\n",
    "#     ax.plot(points[:, 0], points[:, 1], 'ko', label='All Points')\n",
    "\n",
    "#     # Use the same colormap to get consistent colors\n",
    "#     cmap = plt.get_cmap('viridis')\n",
    "#     colors = cmap(np.linspace(0, 1, len(distributions)))\n",
    "\n",
    "#     for dist_idx, point_idx in enumerate(points_to_distribution):\n",
    "#         if point_idx == -1: continue\n",
    "#         ax.plot(points[point_idx, 0], points[point_idx, 1], 'o', color=colors[dist_idx], label=f'Point to Distribution {dist_idx}')\n",
    "    \n",
    "#     ax.legend()\n",
    "#     return plt, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4559e55b-bf9b-4648-aeeb-58f87e67db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def create_ellipse_points(mean, cov, n_std=1.96, n_points=100):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "    angle = np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0])\n",
    "    \n",
    "    t = np.linspace(0, 2*np.pi, n_points)\n",
    "    ellipse_x = n_std * np.sqrt(eigenvalues[0]) * np.cos(t)\n",
    "    ellipse_y = n_std * np.sqrt(eigenvalues[1]) * np.sin(t)\n",
    "    \n",
    "    R = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                  [np.sin(angle), np.cos(angle)]])\n",
    "    \n",
    "    ellipse_points = np.dot(np.column_stack([ellipse_x, ellipse_y]), R.T) + mean\n",
    "    return ellipse_points[:, 0], ellipse_points[:, 1]\n",
    "\n",
    "def plot_confidence_ellipses(distributions, n_std=1.96):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    colors = cycle(cmap(np.linspace(0, 1, len(distributions))))\n",
    "    patches = []\n",
    "\n",
    "    for idx, dist in enumerate(distributions):\n",
    "        mean, cov = dist[0], np.array([dist[1], dist[2]])\n",
    "        x, y = create_ellipse_points(mean, cov, n_std)\n",
    "        \n",
    "        color = next(colors)\n",
    "        \n",
    "        # Create polygon\n",
    "        polygon_points = np.column_stack([x, y])\n",
    "        poly = Polygon(polygon_points, closed=True, fill=True, color=color, alpha=0.5)\n",
    "        ax.add_patch(poly)\n",
    "        patches.append(poly)\n",
    "\n",
    "    ax.set_title('Confidence Ellipses')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend(patches, ['Distribution {}'.format(i) for i in range(len(distributions))])\n",
    "    ax.axis('equal')\n",
    "\n",
    "    return plt, ax\n",
    "\n",
    "def plot_distributions_with_colored_points(distributions, points, points_to_distribution, n_std=1.96):\n",
    "    fig, ax = plot_confidence_ellipses(distributions, n_std)\n",
    "    \n",
    "    # Plot all points in a default color (black)\n",
    "    ax.plot(points[:, 0], points[:, 1], 'ko', label='All Points')\n",
    "\n",
    "    # Use the same colormap to get consistent colors\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    colors = cmap(np.linspace(0, 1, len(distributions)))\n",
    "\n",
    "    for dist_idx, point_idx in enumerate(points_to_distribution):\n",
    "        if point_idx == -1: continue\n",
    "        ax.plot(points[point_idx, 0], points[point_idx, 1], 'o', color=colors[dist_idx], label=f'Point to Distribution {dist_idx}')\n",
    "    \n",
    "    ax.legend()\n",
    "    return plt, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5ce5a03f-f6bf-4aa9-b9c9-781a6bcde4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_from_graph(graph):\n",
    "    node_attributes_a = classes_points_distributions_ids_from_graph(graph)\n",
    "\n",
    "    distributions_a = node_attributes_a[1]\n",
    "    points_a = node_attributes_a[2]\n",
    "    distributions_a[:, :2] +=  points_a[:, :]\n",
    "    \n",
    "    return np.array(distributions_a.reshape(distributions_a.shape[0], -1, 2))\n",
    "\n",
    "def find_mapping_of_two_graphs(graph_a, graph_b, treshhold=0):\n",
    "    node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "    \n",
    "    distributions_a = get_distribution_from_graph(graph_a)\n",
    "    points_b = node_attributes_b[2]\n",
    "    \n",
    "    points_to_distributions = calc_point_indices_to_distributions(distributions_a, points_b, 0)\n",
    "\n",
    "    tensor1 = node_attributes_a[3]\n",
    "    tensor2 = np.where(\n",
    "        points_to_distributions != -1, \n",
    "        node_attributes_b[3][points_to_distributions], \n",
    "        -1\n",
    "    ).flatten()\n",
    "    \n",
    "    return torch.tensor(list(zip(tensor1.tolist(), tensor2.tolist())), dtype=int)\n",
    "\n",
    "def id_mapping_to_idx_mapping(graph_a, graph_b, mapping):\n",
    "    node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "\n",
    "    node_ids_a = node_attributes_a[3]\n",
    "    node_ids_b = node_attributes_b[3]\n",
    "\n",
    "    combined_ids = torch.tensor(list(zip(node_ids_a.tolist(), node_ids_b.tolist())), dtype=int)\n",
    "    \n",
    "    result_tensor = torch.empty_like(combined_ids, dtype=torch.long)\n",
    "\n",
    "    # Fill the first column with index values\n",
    "    result_tensor[:, 0] = torch.arange(combined_ids.size(0))\n",
    "    \n",
    "    # Replace the second column with mapped values\n",
    "    for idx, pair in enumerate(combined_ids):\n",
    "        original_value = pair[1].item()\n",
    "        mapped_value = mapping[mapping[:, 0] == original_value]\n",
    "        if len(mapped_value) > 0:\n",
    "            result_tensor[idx, 1] = mapped_value[0, 1]\n",
    "        else:\n",
    "            result_tensor[idx, 1] = -1\n",
    "\n",
    "    return result_tensor\n",
    "\n",
    "def visualize_graph_idx_mapping(graph_a, graph_b, mapping, n_std=1.96):\n",
    "    node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "\n",
    "    distributions_a = get_distribution_from_graph(graph_a)\n",
    "    points_b = node_attributes_b[2]\n",
    "\n",
    "    fig, ax = plot_confidence_ellipses(distributions_a, n_std)\n",
    "    \n",
    "    # Plot all points in a default color (black)\n",
    "    ax.plot(points_b[:, 0], points_b[:, 1], 'ko', label='All Points')\n",
    "\n",
    "    # Use the same colormap to get consistent colors\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    colors = cmap(np.linspace(0, 1, len(distributions_a)))\n",
    "\n",
    "    for dist_idx, point_idx in mapping:\n",
    "        if point_idx == -1: continue\n",
    "        ax.plot(points_b[point_idx, 0], points_b[point_idx, 1], 'o', color=colors[dist_idx], label=f'Point to Distribution {dist_idx}')\n",
    "    \n",
    "    ax.legend()\n",
    "    return plt, ax\n",
    "\n",
    "# for i in range(500, 550):\n",
    "#     print(i)\n",
    "#     graph_a = train_dataset[i]\n",
    "#     graph_b = train_dataset[i+1]\n",
    "\n",
    "#     id_mapping = find_mapping_of_two_graphs(graph_a, graph_b)\n",
    "#     idx_mapping = id_mapping_to_idx_mapping(graph_a, graph_b, id_mapping)\n",
    "#     visualize_graph_idx_mapping(graph_a, graph_b, idx_mapping)\n",
    "#     time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b6a1b6-846c-4be9-a291-750e7737cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hashlib import sha256\n",
    "\n",
    "def get_name_and_boxes_for_iteration(csv_file_name, iteration):\n",
    "    names = []\n",
    "    boxes = []\n",
    "    \n",
    "    with open(csv_file_name, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if int(row['iteration']) == iteration:\n",
    "                names.append(row['class'])\n",
    "                boxes.append({\n",
    "                    'top': float(row['top']),\n",
    "                    'left': float(row['left']),\n",
    "                    'bottom': float(row['bottom']),\n",
    "                    'right': float(row['right']),\n",
    "                    'center_x': float(row['center_x']),\n",
    "                    'center_y': float(row['center_y']),\n",
    "                    'object_id': int(row['object_id'])\n",
    "                })\n",
    "    \n",
    "    return names, boxes\n",
    "\n",
    "def generate_color(object_id):\n",
    "    \"\"\"\n",
    "    Generate a unique color for each object_id.\n",
    "    Uses a hash function to create unique colors.\n",
    "    \"\"\"\n",
    "    hash_object = sha256(str(object_id).encode())\n",
    "    hex_color = hash_object.hexdigest()[:6]\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def draw_boxes_to_canvas(csv_file_name, image_widget, x_limit=240, y_limit=240, frame_delay=0.01):\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    num_iterations = int(df.iloc[-1]['iteration']) + 1\n",
    "    for iteration in range(num_iterations):\n",
    "        names, boxes = get_name_and_boxes_for_iteration(csv_file_name, iteration)\n",
    "        \n",
    "        height, width = y_limit, x_limit\n",
    "        canvas = np.ones((height, width, 3), dtype=np.uint8) * 255  # Create a white background\n",
    "        \n",
    "        # Draw the file name in the top-left corner\n",
    "        file_name_text = f\"File: {os.path.basename(csv_file_name)}\"\n",
    "        cv2.putText(canvas, file_name_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        \n",
    "        for name, box in zip(names, boxes):\n",
    "            top = int(box['top'])\n",
    "            left = int(box['left'])\n",
    "            bottom = int(box['bottom'])\n",
    "            right = int(box['right'])\n",
    "            color = generate_color(box['object_id'])  # Get unique color for object_id\n",
    "            \n",
    "            cv2.rectangle(canvas, (top, left), (bottom, right), color, 2)\n",
    "            cv2.putText(canvas, name, (top, left - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        _, buffer = cv2.imencode('.jpg', canvas)\n",
    "        image_widget.value = buffer.tobytes()\n",
    "\n",
    "        time.sleep(frame_delay)\n",
    "\n",
    "def replay_csv_files_from_directory(directory_name, image_widget):\n",
    "    for file_name in sorted(os.listdir(directory_name)):\n",
    "        if file_name.endswith('.csv'):\n",
    "            csv_file_path = os.path.join(directory_name, file_name)\n",
    "            draw_boxes_to_canvas(csv_file_path, image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df746-fbbf-4bf5-b06c-56c19a81201f",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df621322-06c3-4565-b0eb-30fa4a010904",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2934e962-d3bc-472f-b4a4-a71ebc4ca432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/jupyterlite/content/pytroch-geometric/helper.py:73: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.linalg.norm(val_edge_pairs, axis=1), np.rad2deg(np.arctan(val_edge_pairs[:, 1] / val_edge_pairs[:, 0]))\n",
      "/workspaces/jupyterlite/content/pytroch-geometric/helper.py:73: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.linalg.norm(val_edge_pairs, axis=1), np.rad2deg(np.arctan(val_edge_pairs[:, 1] / val_edge_pairs[:, 0]))\n",
      "/usr/local/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for i in range(4):\n",
    "    csv_file_name = f\"/workspaces/jupyterlite/content/pytroch-geometric/mario-tracking-data/{i:04d}.csv\"\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    num_iterations = int(df.iloc[-1]['iteration']) # we start at index 1, so no need for `+ 1`\n",
    "    \n",
    "    for iteration in range(1, num_iterations):\n",
    "        graph_data = create_graph_data(csv_file_name, iteration)\n",
    "        dataset.append(graph_data)\n",
    "\n",
    "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
    "test_dataset = dataset[int(0.8 * len(dataset)):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6429f-47a6-41f1-b38b-7b6bbb0e0428",
   "metadata": {},
   "source": [
    "#### Message Passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdce48fe-779c-488e-aa99-209e93d8f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=1, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index, target_node):\n",
    "        out = self.conv1(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96696cf-f16c-4c69-aca9-e78d32cf2226",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b181b4-a928-48a1-89e5-136027d6c97e",
   "metadata": {},
   "source": [
    "The Loss will be the SSE of the likelihoods that the points of frame i+1 belong to the distributions that were created from the graph on frame i.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6612f0ae-24af-4d90-ab17-62a9601f68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_a = train_dataset[729]\n",
    "# graph_b = train_dataset[730]\n",
    "# mappings = find_mapping_of_two_graphs(graph_a, graph_b)\n",
    "# # calculate_sse_for_distributions_and_points(graph_a, graph_b, mappings)\n",
    "# mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e2e7bf-4787-4c61-b81b-d8350a847503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 181,pipe,184.03,176.08,215.29,207.41,191.74,199.66,5\n",
    "# 181,goomba,40.02,192.69,55.04,208.47,200.58,47.53,3\n",
    "# 181,box,54.96,144.2,135.95,158.78,151.49,95.46,1\n",
    "# 181,mario,111.22,171.09,126.66,186.69,178.89,118.94,0\n",
    "# 181,box,88.64,80.0,103.0,94.96,87.48,95.82,2\n",
    "# 181,box,0.17,143.9,7.0,158.91,151.4,3.58,4\n",
    "\n",
    "# 182,box,52.66,144.18,132.96,158.82,151.5,92.81,1\n",
    "# 182,mario,111.83,175.04,126.71,190.72,182.88,119.27,0\n",
    "# 182,goomba,35.98,192.72,51.03,208.52,200.62,43.51,3\n",
    "# 182,pipe,180.88,176.06,211.88,207.4,191.73,196.38,5\n",
    "# 182,box,85.57,79.99,100.01,94.97,87.48,92.79,2\n",
    "# 182,box,0.15,144.0,4.11,159.0,151.5,2.13,4\n",
    "\n",
    "# 183,pipe,177.88,176.09,209.32,207.39,191.74,193.6,5\n",
    "# 183,box,82.59,80.0,97.04,94.97,87.49,89.82,2\n",
    "# 183,goomba,32.99,192.8,48.09,208.56,200.68,40.54,3\n",
    "# 183,box,49.14,144.22,129.9,158.8,151.51,89.52,1\n",
    "# 183,mario,112.29,180.27,126.62,194.99,187.63,119.46,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4322b-b188-4022-ac74-1b362127e01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3b6b4e-7a51-43fd-b7fa-eca2928e7567",
   "metadata": {},
   "source": [
    "#### Graph to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9e7a754-abe8-4d0a-8e0a-c6b04b172064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_graph(graph):\n",
    "    return Data(\n",
    "        x=graph.x.clone(),\n",
    "        edge_index=graph.edge_index.clone(),\n",
    "        edge_attr=graph.edge_attr.clone() if graph.edge_attr is not None else None\n",
    "    )\n",
    "\n",
    "def duplicate_dataset(dataset):\n",
    "    return [duplicate_graph(graph) for graph in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee826a0-4b72-48d7-af78-ea4db26fe13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in train_dataset:\n",
    "    csv_entry_for_graph(graph, 'graph_to_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e5a358c-85b5-4d8f-8c55-79a3406d53f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1036390/2994302452.py:26: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return torch.tensor(list(zip(tensor1.tolist(), tensor2.tolist())), dtype=int)\n"
     ]
    }
   ],
   "source": [
    "two_graph_prediction_trainingset = duplicate_dataset(train_dataset)\n",
    "for i in range(0, 298):\n",
    "    graph_a = two_graph_prediction_trainingset[i]\n",
    "    graph_b = two_graph_prediction_trainingset[i+1]\n",
    "    graph_id_tracking(graph_a, graph_b)\n",
    "    csv_entry_for_graph(graph_a, 'graph_to_csv-applied_tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8d489-f41e-4678-8d58-6ade8ff48d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def graph_id_tracking(graph_a, graph_b, treshhold=0):\n",
    "#     global filter_max_label_number\n",
    "#     mappings = find_mapping_of_two_graphs(graph_a, graph_b, treshhold)\n",
    "    \n",
    "#     all_node_ids = torch.tensor([int(node_entry[-1]) for node_entry in graph_b.x])\n",
    "#     not_tracked_nodes = all_node_ids[~all_node_ids.unsqueeze(-1).eq( mappings[:, 1]).any(dim=-1)]\n",
    "\n",
    "#     indicies_graph_b = graph_b.x[:, -1]\n",
    "#     indicies_graph_a = graph_a.x[:, -1]\n",
    "#     max_graph_value = max(max(indicies_graph_a), max(indicies_graph_b))\n",
    "\n",
    "#     # if (graph_b.x[0, -2] > 127): sys.exit(-1)\n",
    "#     for i, object_id in enumerate(indicies_graph_b):\n",
    "#         if(object_id in not_tracked_nodes):\n",
    "#             graph_b.x[i, -1] = filter_max_label_number\n",
    "#             # print('filter_max_label_number', filter_max_label_number)\n",
    "#             filter_max_label_number += 1\n",
    "#         else:\n",
    "#             # mapping_idx = int(torch.where(mappings[:, 1] == int(object_id))[0]) \n",
    "#             mapping_idx_tensor = torch.where(mappings[:, 1] == object_id)[0]\n",
    "#             mapping_idx = mapping_idx_tensor.item()\n",
    "#             graph_b.x[i, -1] = mappings[mapping_idx, 0]\n",
    "#             # print('mappings', mappings[mapping_idx, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eb07fc67-55be-4d09-b3ee-ba9662d53eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_point_indices_to_gmm(distributions, ids, weights, points, treshhold_likelihood=0):\n",
    "    # unique_ids = np.unique(np.concatenate(ids))\n",
    "    # id_to_order = {id_val: index for index, id_val in enumerate(unique_ids)}\n",
    "    # # id_ordering = np.vectorize(id_to_order.get)(ids)\n",
    "    # id_ordering = [np.array([id_to_order[id_val] for id_val in arr]) for arr in ids]\n",
    "    \n",
    "    # likelihoods_time_distributions_to_points = np.zeros((len(unique_ids), points.shape[0]))\n",
    "    \n",
    "    # for i, iteration_distributions in enumerate(distributions):\n",
    "    #     likelihood_entries, likelihood_indicies_to_filter = calc_likelihood_entries_to_distributions(iteration_distributions, points)\n",
    "    #     likelihood_entries[likelihood_indicies_to_filter] = 0\n",
    "    #     likelihoods_time_distributions_to_points[id_ordering[0, :]] += weights[i] * likelihood_entries.T\n",
    "\n",
    "    unique_ids = np.unique(np.concatenate(ids))\n",
    "    id_to_order = {id_val: index for index, id_val in enumerate(unique_ids)}\n",
    "    # id_ordering = np.vectorize(id_to_order.get)(ids)\n",
    "    id_ordering = [np.array([id_to_order[id_val] for id_val in arr]) for arr in ids]\n",
    "    \n",
    "    likelihoods_time_distributions_to_points = np.zeros((len(unique_ids), points.shape[0]))\n",
    "    \n",
    "    for i, iteration_distributions in enumerate(distributions):\n",
    "        likelihood_entries, likelihood_indicies_to_filter = calc_likelihood_entries_to_distributions(iteration_distributions, points)\n",
    "        likelihood_entries[likelihood_indicies_to_filter] = 0\n",
    "        likelihoods_time_distributions_to_points[id_ordering[i]] += weights[i] * likelihood_entries.T\n",
    "\n",
    "    likelihoods_time_distributions_to_points[likelihoods_time_distributions_to_points == 0] = 1e-50\n",
    "    point_mappings = calc_point_indicies_to_likelihoods_no_filter(likelihoods_time_distributions_to_points)\n",
    "    for i, point_mapping in enumerate(point_mappings):\n",
    "        if(point_mapping == -1): continue\n",
    "        point_mappings[i] = unique_ids[point_mappings[i]]\n",
    "    \n",
    "    return point_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "935b021f-1895-4fa4-99fb-28f59d55ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights_that_decay(num):\n",
    "    return (1/np.arange(1, num+1)**(1.5))/np.sum((1/np.arange(1, num+1)**(1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8970e7f2-2f75-4e08-85ee-72333d683478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mapping_multiple_graphs(tracked_graphs, untracked_graph):\n",
    "    num_tracked_graphs = len(tracked_graphs)\n",
    "    weights = calc_weights_that_decay(num_tracked_graphs)\n",
    "    node_attributes_points = classes_points_distributions_ids_from_graph(untracked_graph)\n",
    "    \n",
    "    distributions = [get_distribution_from_graph(graphs) for graph in tracked_graphs]\n",
    "    ids = [np.array([int(val) for val in graph.x[:, -1].tolist()]) for graph in untracked_graphs]\n",
    "    points = node_attributes_points[2]\n",
    "    \n",
    "    return calc_point_indices_to_gmm(distributions, ids, weights, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6aa8b8a6-fcdf-45bf-9d93-36adf24476c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_graph_id_tracking(graphs):\n",
    "    tracked_graphs = graphs[:-1]\n",
    "    untracked_graph = graphs[-1]\n",
    "\n",
    "    id_for_points = find_mapping_multiple_graphs(tracked_graphs, untracked_graph)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "50aa8afa-1c3d-4055-ba8b-d0bbcdb1c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 3])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graphs = [train_dataset[100], train_dataset[101], train_dataset[102], train_dataset[103]]\n",
    "\n",
    "# # we assume that the past id's are tracked correctly\n",
    "# num_iterations = len(graphs) - 1\n",
    "# weights = calc_weights_that_decay(num_iterations)\n",
    "# node_attributes_points = classes_points_distributions_ids_from_graph(graphs[-1])\n",
    "\n",
    "# distributions = [get_distribution_from_graph(graphs[i]) for i in range(len(graphs) - 1)]\n",
    "# # ids = [graphs[i].x[:, -1].numpy() for i in range(len(graphs) - 1)]\n",
    "# ids = [np.array([int(val) for val in graphs[i].x[:, -1].tolist()]) for i in range(len(graphs) -1)]\n",
    "\n",
    "# # distributions = [graphs[i].x[:, 0] for i in range(len(graphs) - 1)]\n",
    "# # ids = [int(graphs[i].x[:, -1][0]) for i in range(len(graphs) - 1)]\n",
    "\n",
    "# points = node_attributes_points[2]\n",
    "\n",
    "# calc_point_indices_to_gmm(distributions, ids, weights, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dce4b8-8b59-4264-acdc-2cc3eb681c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_graph_prediction_trainingset = duplicate_dataset(train_dataset)\n",
    "for i in range(0, 298):\n",
    "    min_val = max(0, i-(5-1))\n",
    "    graphs = [five_graph_prediction_trainingset[j] for j in range(min_val, i)]\n",
    "    multi_graph_id_tracking(graphs)\n",
    "    csv_entry_for_graph(graph_a, 'graph_to_csv-applied_tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37c035-c55e-4e39-9fab-6699bb74d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color_for_idx(object_id):\n",
    "    \"\"\"\n",
    "    Generate a unique color for each object_id using a hash function.\n",
    "    \"\"\"\n",
    "    hash_object = sha256(str(object_id).encode())\n",
    "    hex_color = hash_object.hexdigest()[:6]\n",
    "    return tuple(int(hex_color[i:i+2], 16) / 255.0 for i in (0, 2, 4))\n",
    "\n",
    "def plot_confidence_ellipses_with_ids(distributions, ids, n_std=1.96):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    all_x, all_y = [], []\n",
    "    patches = []\n",
    "    for idx, dist in enumerate(distributions):\n",
    "        mean, cov = dist[0], np.array([dist[1], dist[2]])\n",
    "        x, y = create_ellipse_points(mean, cov, n_std)\n",
    "    \n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "        \n",
    "        color = generate_color_for_idx(ids[idx])\n",
    "        \n",
    "        polygon_points = np.column_stack([x, y])\n",
    "        poly = Polygon(polygon_points, closed=True, fill=True, color=color, alpha=0.5)\n",
    "        ax.add_patch(poly)\n",
    "        patches.append(poly)\n",
    "    \n",
    "    ax.set_xlim(0, 250)\n",
    "    ax.set_ylim(0, 250)\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    \n",
    "    return plt, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf0fb3a-e9ae-4075-93fe-70fb24177cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_cache = []\n",
    "\n",
    "for i in range(0, 298):\n",
    "    graph_a = train_dataset[i]\n",
    "    graph_b = train_dataset[i+1]\n",
    "    fig, ax = end_to_end_visualize_graphs_by_distributions_and_points(graph_a, graph_b)\n",
    "\n",
    "    rotated_image = rotate_plot(fig, 270)\n",
    "    fig.close()\n",
    "    images_cache.append(rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ee652-f584-4428-a7af-f35c6b7d5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_and_play_html_video(images_cache, size=480, dpi=300, interpolation='lanczos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ba7eb-4322-4e9b-8655-705d73dcf234",
   "metadata": {},
   "source": [
    "#### Visualize the CSV-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97625208-3df2-4287-8a57-f4630b6c30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import time\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg')\n",
    "display.display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574df8a6-3a3a-4ea7-8554-7941d02f94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_name = \"/workspaces/jupyterlite/workspace/supermario_graph-nn/03_supermario_graph-nn/yolo_results/clean_multiple_episodes\"\n",
    "# replay_csv_files_from_directory(directory_name, image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2ab3f-b05f-4e5b-a19f-cc5fb6754ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_name = \"/workspaces/jupyterlite/content/pytroch-geometric/graph_to_csv\"\n",
    "# replay_csv_files_from_directory(directory_name, image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876e4b3-bebb-482f-bc10-7ebe739f3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = \"/workspaces/jupyterlite/content/pytroch-geometric/graph_to_csv-applied_tracking\"\n",
    "replay_csv_files_from_directory(directory_name, image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fccc4b5-8584-4397-b367-9bfa7b5bd2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b65246-0188-4246-b7c0-f0cf65210e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6fb21-fd4e-41fc-b552-1210cd76a187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
