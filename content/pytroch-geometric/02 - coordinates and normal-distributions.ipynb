{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe4a48d-b92d-4398-8702-b72be6c8f7e3",
   "metadata": {},
   "source": [
    "### Imports and HTML-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e013a98-618b-4936-9dd9-dbca51355186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bf5323-ff7a-4905-bda6-15f9f7b16d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "point_to_dist_angle = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52fcb20c-672e-4ea1-a387-442a16fa7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def calc_likelihoods_for_distributions_and_points(matrix_dist, matrix_points):\n",
    "    means = matrix_dist[:, 0]\n",
    "    covariances = matrix_dist[:, 1:]\n",
    "    likelihoods = np.array([multivariate_normal(mean=means[i], cov=covariances[i]).pdf(matrix_points) for i in range(len(means))])\n",
    "    \n",
    "    return likelihoods.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb3469-3fc7-467a-956f-d3760fd28d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d24df746-fbbf-4bf5-b06c-56c19a81201f",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df621322-06c3-4565-b0eb-30fa4a010904",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4153ff8-aae4-4dfe-ab70-a35a401bc6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@save_params\n",
    "def create_graph_data(csv_file_name, iteration):\n",
    "    classes_dict = {\n",
    "        'goomba': 0,\n",
    "        'mario': 1,\n",
    "        'cloud': 2,\n",
    "        'ground': 3,\n",
    "        'bush': 4,\n",
    "        'box': 5,\n",
    "        'pipe': 6\n",
    "    }\n",
    "    \n",
    "    class_names, boxes = get_classnames_boxes_from_csv(csv_file_name, iteration)\n",
    "    num_nodes = len(boxes)\n",
    "    edge_connections = cartesian_product_for_nodes(range(num_nodes))\n",
    "    node_features = []\n",
    "    matrix = np.empty((0, 2))\n",
    "    normal_dist = [0, 0, 1, 0, 0, 1] # mu1, mu2, sig00, sig01, sig10, sig11\n",
    "    dataset_number = int(csv_file_name.split('/')[-1].split('.')[0])\n",
    "\n",
    "    if num_nodes == 1:\n",
    "        box = boxes[0]\n",
    "        width, height = abs(box['left'] - box['right']), abs(box['top'] - box['bottom'])\n",
    "        # node_feature: (normal-distribution, class-label, x-val, y-val, width, height, dataset_number, iteration, id)\n",
    "        node_features.append((*normal_dist, classes_dict[class_names[0]], box['center_x'], \\\n",
    "                              box['center_y'], width, height, dataset_number, iteration, 0))\n",
    "        \n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        return Data(\n",
    "            x=node_features,\n",
    "            edge_index=torch.tensor([0]),\n",
    "            edge_attr=torch.tensor([0])\n",
    "        )\n",
    "        \n",
    "    for i, box in enumerate(boxes):\n",
    "        new_row = np.array([[box['center_x'], box['center_y']]])\n",
    "        matrix = np.vstack((matrix, new_row))\n",
    "\n",
    "        width, height = abs(box['left'] - box['right']), abs(box['top'] - box['bottom'])\n",
    "        # node_feature: (normal-distribution, class-label, x-val, y-val, width, height, dataset_number, iteration, id)\n",
    "        node_features.append((*normal_dist, classes_dict[class_names[i]], box['center_x'], \\\n",
    "                              box['center_y'], width, height, dataset_number, iteration, i))\n",
    "    \n",
    "    dists, angles = dist_angle_from_matrix(matrix, edge_connections)\n",
    "    \n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_connections = torch.tensor(edge_connections)\n",
    "    edges_features =  torch.tensor(np.stack((dists, angles), axis=-1))\n",
    "    \n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_connections.t().contiguous(),\n",
    "        edge_attr=edges_features\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2934e962-d3bc-472f-b4a4-a71ebc4ca432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/jupyterlite/content/pytroch-geometric/helper.py:73: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.linalg.norm(val_edge_pairs, axis=1), np.rad2deg(np.arctan(val_edge_pairs[:, 1] / val_edge_pairs[:, 0]))\n",
      "/workspaces/jupyterlite/content/pytroch-geometric/helper.py:73: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.linalg.norm(val_edge_pairs, axis=1), np.rad2deg(np.arctan(val_edge_pairs[:, 1] / val_edge_pairs[:, 0]))\n",
      "/usr/local/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for i in range(4):\n",
    "    csv_file_name = f\"/workspaces/jupyterlite/content/pytroch-geometric/mario-tracking-data/{i:04d}.csv\"\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    num_iterations = int(df.iloc[-1]['iteration']) # we start at index 1, so no need for `+ 1`\n",
    "    \n",
    "    for iteration in range(1, num_iterations):\n",
    "        graph_data = create_graph_data(csv_file_name, iteration)\n",
    "        dataset.append(graph_data)\n",
    "\n",
    "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
    "test_dataset = dataset[int(0.8 * len(dataset)):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2d2b08-8315-4d0e-bd8b-eb8c2c5a8f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(csv_file_name.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6429f-47a6-41f1-b38b-7b6bbb0e0428",
   "metadata": {},
   "source": [
    "#### Message Passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdce48fe-779c-488e-aa99-209e93d8f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=1, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, edge_index, target_node):\n",
    "        out = self.conv1(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96696cf-f16c-4c69-aca9-e78d32cf2226",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b181b4-a928-48a1-89e5-136027d6c97e",
   "metadata": {},
   "source": [
    "The Loss will be the SSE of the likelihoods that the points of frame i+1 belong to the distributions that were created from the graph on frame i.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb6ca06-782a-4c52-a615-69685de770f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_point_indices_to_distributions(distributions, points, treshhold_likelihood = 0):\n",
    "    if points.shape[0] == 0:\n",
    "        return np.zeros(distributions.shape[0]) - 1\n",
    "    \n",
    "    if points.shape[0] == 1 and distributions.shape[0] == 1:\n",
    "        likelihood_entries = calc_likelihoods_for_distributions_and_points(distributions, points)\n",
    "    \n",
    "        if likelihood_entries < treshhold_likelihood: return np.array([-1])\n",
    "        return np.array([0])\n",
    "\n",
    "    likelihoods = calc_likelihoods_for_distributions_and_points(distributions, points)\n",
    "    sorted_indices = np.argsort(likelihoods, axis=0)\n",
    "    ranks = np.zeros_like(likelihoods, dtype=int)\n",
    "    n_rows, n_cols = likelihoods.shape\n",
    "    ranks[sorted_indices, np.arange(n_cols)] = np.tile(np.arange(n_rows), (n_cols, 1)).T\n",
    "    \n",
    "    mask_binary = np.array((n_rows - ranks) <= n_cols, dtype=int)\n",
    "    cumsum_array = np.cumsum(mask_binary, axis=0)\n",
    "    \n",
    "    s = min(n_rows, n_cols)\n",
    "    extended_likelihood_entries = np.zeros((s+1, n_cols + 1))\n",
    "    \n",
    "    mask  = np.array((n_rows - ranks) <= n_cols)\n",
    "    \n",
    "    points_to_consider = np.where(np.any(mask, axis=1))[0]\n",
    "    filtered_points = points[points_to_consider]\n",
    "    \n",
    "    likelihood_entries = calc_likelihoods_for_distributions_and_points(distributions, filtered_points)\n",
    "    likelihood_indicies_to_filter = likelihood_entries < treshhold_likelihood\n",
    "    # punish points that should be filtered out\n",
    "    likelihood_entries[likelihood_indicies_to_filter] = 1e-50\n",
    "    \n",
    "    inv_percentage_of_likelihood = np.log(likelihood_entries)/ np.sum(np.log(likelihood_entries), axis=0)\n",
    "    # percentage_of_likelihood = (1 / inv_percentage_of_likelihood) / np.sum( (1 / inv_percentage_of_likelihood), axis=0)\n",
    "    # percentage_of_likelihood = inv_percentage_of_likelihood\n",
    "    \n",
    "    cost_matrix = np.log(inv_percentage_of_likelihood)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    max_index = max(col_ind) + 1\n",
    "    points_to_distributions = np.zeros(max_index, dtype=row_ind.dtype) - 1\n",
    "    \n",
    "    points_to_distributions[col_ind] = row_ind\n",
    "\n",
    "    # if punished points still come up ahead, set the association to -1\n",
    "    for point_index, distribution_index in enumerate(col_ind):\n",
    "        if likelihood_indicies_to_filter[point_index, distribution_index]:\n",
    "            points_to_distributions[point_index] = -1\n",
    "    \n",
    "    return points_to_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c442a1-df47-4998-891b-a60afd83ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_points_distributions_ids_from_graph(graph):\n",
    "    graph = graph.x\n",
    "    return graph[:, 6], graph[:, :6], graph[:, 7:9], graph[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314d57a8-4134-49ce-9a14-2d0aa85d3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimial_matching_for_distributions_and_points(graph, csv_file_name, iteration, threshhold=0):\n",
    "        classes_dict = {\n",
    "        'goomba': 0,\n",
    "        'mario': 1,\n",
    "        'cloud': 2,\n",
    "        'ground': 3,\n",
    "        'bush': 4,\n",
    "        'box': 5,\n",
    "        'pipe': 6\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904ee05d-cbf2-4db3-826d-4cab990a1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicies_of_filterted_array_entries(filtered_indicies, relative_indicies):\n",
    "    return np.where(filtered_indicies)[0][relative_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10e8cde-6e1a-43e2-9027-7057fd52315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sse_for_distributions_and_points(graph_a, graph_b, mappign):\n",
    "    node_attr_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attr_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "\n",
    "    # likelihoods = calc_likelihoods_for_distributions_and_points(node_attr_a[1][mappings[0, :], node_attr_b[2][mappings[1, :])\n",
    "    \n",
    "    # return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101935ee-d25e-4de3-8f5c-de8cabc581f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @save_params\n",
    "# def find_mapping_of_two_graphs(graph_a, graph_b):\n",
    "#     node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "#     node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "\n",
    "#     total_mappings = torch.empty((0, 2), dtype=torch.int)\n",
    "\n",
    "#     for label in classes_points_distributions_ids_from_graph(graph_b)[0].unique():\n",
    "#         dist_filter = (node_attributes_a[0] == label)\n",
    "#         dist_indicies = np.copy(dist_filter)\n",
    "#         points_filter = (node_attributes_b[0] == label)\n",
    "#         point_indicies = np.copy(points_filter)\n",
    "#         distributions = node_attributes_a[1][dist_indicies]\n",
    "#         distributions = np.array(distributions.reshape(distributions.shape[0], -1, 2)) # to have the correct format\n",
    "#         distributions[:, 0, :] += np.array(node_attributes_a[2][dist_indicies])\n",
    "#         points = node_attributes_b[2][point_indicies]\n",
    "#         num_dist_entries = torch.sum(dist_filter)\n",
    "        \n",
    "#         mappings = torch.zeros((num_dist_entries, 2)) - 1\n",
    "#         mappings[:, 0] = torch.tensor((node_attributes_a[-1])[dist_filter], dtype=torch.int)\n",
    "        \n",
    "#         dist_to_point_mapping = calc_point_indices_to_distributions(distributions, points)\n",
    "        \n",
    "#         for i, dest_index in enumerate(dist_to_point_mapping):\n",
    "#             if dest_index == -1: continue\n",
    "#             relative_point_index = indicies_of_filterted_array_entries(points_filter, i)\n",
    "#             mappings[dest_index, 1] = torch.tensor((node_attributes_b[-1])[relative_point_index], dtype=torch.int)\n",
    "\n",
    "#         total_mappings = torch.cat((total_mappings, mappings), dim=0)\n",
    "\n",
    "#     return total_mappings\n",
    "\n",
    "@save_params\n",
    "def find_mapping_of_two_graphs(graph_a, graph_b):\n",
    "    node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "    node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "\n",
    "    total_mappings = torch.empty((0, 2), dtype=torch.int)\n",
    "\n",
    "    for label in classes_points_distributions_ids_from_graph(graph_b)[0].unique():\n",
    "        dist_filter = (node_attributes_a[0] == label)\n",
    "        dist_indicies = np.copy(dist_filter)\n",
    "        points_filter = (node_attributes_b[0] == label)\n",
    "        point_indicies = np.copy(points_filter)\n",
    "        distributions = node_attributes_a[1][dist_indicies]\n",
    "        distributions = np.array(distributions.reshape(distributions.shape[0], -1, 2))  # to have the correct format\n",
    "        distributions[:, 0, :] += np.array(node_attributes_a[2][dist_indicies])\n",
    "        points = node_attributes_b[2][point_indicies]\n",
    "        num_dist_entries = torch.sum(dist_filter)\n",
    "        \n",
    "        mappings = torch.full((num_dist_entries, 2), -1, dtype=torch.int)\n",
    "        mappings[:, 0] = torch.tensor((node_attributes_a[-1])[dist_filter], dtype=torch.int)\n",
    "        \n",
    "        dist_to_point_mapping = calc_point_indices_to_distributions(distributions, points)\n",
    "        \n",
    "        for i, dest_index in enumerate(dist_to_point_mapping):\n",
    "            if dest_index == -1:\n",
    "                continue\n",
    "            relative_point_index = indicies_of_filterted_array_entries(points_filter, i)\n",
    "            mappings[dest_index, 1] = torch.tensor((node_attributes_b[-1])[relative_point_index], dtype=torch.int)\n",
    "        \n",
    "        total_mappings = torch.cat((total_mappings, mappings), dim=0)\n",
    "\n",
    "    return total_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba394013-47c4-412e-be3e-112278a892de",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_a = train_dataset[700]\n",
    "graph_b = train_dataset[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b47f844-f0b7-4c4e-a6b3-1e58013a24d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_629158/1774780086.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mappings[:, 0] = torch.tensor((node_attributes_a[-1])[dist_filter], dtype=torch.int)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'points' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfind_mapping_of_two_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_b\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/jupyterlite/content/pytroch-geometric/helper.py:28\u001b[0m, in \u001b[0;36msave_params.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames, args)})\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m, in \u001b[0;36mfind_mapping_of_two_graphs\u001b[0;34m(graph_a, graph_b)\u001b[0m\n\u001b[1;32m     51\u001b[0m mappings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((num_dist_entries, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m     52\u001b[0m mappings[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor((node_attributes_a[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])[dist_filter], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m---> 54\u001b[0m dist_to_point_mapping \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_point_indices_to_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dest_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dist_to_point_mapping):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dest_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mcalc_point_indices_to_distributions\u001b[0;34m(distributions, points, treshhold_likelihood)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(distributions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m distributions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     likelihood_entries \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_likelihoods_for_distributions_and_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m likelihood_entries \u001b[38;5;241m<\u001b[39m treshhold_likelihood: \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mcalc_likelihoods_for_distributions_and_points\u001b[0;34m(matrix_dist, matrix_points)\u001b[0m\n\u001b[1;32m      4\u001b[0m means \u001b[38;5;241m=\u001b[39m matrix_dist[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m covariances \u001b[38;5;241m=\u001b[39m matrix_dist[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 6\u001b[0m likelihoods \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([multivariate_normal(mean\u001b[38;5;241m=\u001b[39mmeans[i], cov\u001b[38;5;241m=\u001b[39mcovariances[i])\u001b[38;5;241m.\u001b[39mpdf(points) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(means))])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m likelihoods\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m means \u001b[38;5;241m=\u001b[39m matrix_dist[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m covariances \u001b[38;5;241m=\u001b[39m matrix_dist[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 6\u001b[0m likelihoods \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([multivariate_normal(mean\u001b[38;5;241m=\u001b[39mmeans[i], cov\u001b[38;5;241m=\u001b[39mcovariances[i])\u001b[38;5;241m.\u001b[39mpdf(\u001b[43mpoints\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(means))])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m likelihoods\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mNameError\u001b[0m: name 'points' is not defined"
     ]
    }
   ],
   "source": [
    "find_mapping_of_two_graphs(graph_a, graph_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d0773-c5dd-471e-889e-564237dd6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_a.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463f477-4270-4842-b06e-fda385432449",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55d211-67ba-4c21-985d-d43cfcff027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappigs = find_mapping_of_two_graphs(graph_a, graph_b)\n",
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5ce3b-b7f7-4275-9ec2-5e4bd9ad7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_b.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58d3eb-8714-47e2-8d0e-0b0f4f24843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[mappings[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1272496-63af-4716-8bb3-7dfdd6f17890",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = {\n",
    "    'goomba': 0,\n",
    "    'mario': 1,\n",
    "    'cloud': 2,\n",
    "    'ground': 3,\n",
    "    'bush': 4,\n",
    "    'box': 5,\n",
    "    'pipe': 6\n",
    "}\n",
    "\n",
    "csv_data = get_classnames_boxes_from_csv(csv_file_name, 50)\n",
    "class_ids = np.array([classes_dict[class_name] for class_name in csv_data[0]])\n",
    "boxes_coordinates = np.empty((0, 2))\n",
    "\n",
    "for box_values in csv_data[1]:\n",
    "    box_coordinates =  np.array([box_values['center_x'], box_values['center_y']])\n",
    "    boxes_coordinates = np.vstack((boxes_coordinates, box_coordinates))\n",
    "\n",
    "class_ids, boxes_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860fcff-2aa6-43cb-a1ed-fd9db5342a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426c171-206a-4925-bdd3-13aa2ece5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes_a = classes_points_distributions_ids_from_graph(graph_a)\n",
    "node_attributes_b = classes_points_distributions_ids_from_graph(graph_b)\n",
    "node_attributes_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5e80a-a93b-4875-bded-06f1cdf97bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_of_distributions_from_center_of_distributions_to_points(distributions, center_distributions, points):\n",
    "    normalized_points = points - center_distribution\n",
    "    return calc_point_indices_to_distributions(distributions, normalized_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6c89d-76ec-44d0-8d64-464743198e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 2, 1, 1, 3])\n",
    "matrix = np.array([\n",
    "    [1, 1],\n",
    "    [2, 3],\n",
    "    [4, 1],\n",
    "    [1, 4],\n",
    "    [5, 3]\n",
    "])\n",
    "\n",
    "matrix[labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a295753-f7d1-4f8b-b84a-2347d543313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_points_distributions_ids_from_graph(graph_b)[0].unique()\n",
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c2181-34fb-4d37-97b6-1b375e951fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_nodes = torch.empty((0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab204ea5-5a58-4412-8970-bf69d21e5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in node_attributes_a[0].unique():\n",
    "label = 1\n",
    "dist_filter = (node_attributes_a[0] == label)\n",
    "dist_indicies = np.copy(dist_filter)\n",
    "points_filter = (node_attributes_b[0] == label)\n",
    "point_indicies = np.copy(points_filter)\n",
    "distributions = node_attributes_a[1][dist_indicies]\n",
    "distributions = np.array(distributions.reshape(distributions.shape[0], -1, 2)) # to have the correct format\n",
    "distributions[:, 0, :] += np.array(node_attributes_a[2][point_indicies])\n",
    "points = node_attributes_b[2][point_indicies]\n",
    "num_dist_entries = torch.sum(dist_filter)\n",
    "\n",
    "mappings = torch.zeros((num_dist_entries, 2)) - 1\n",
    "mappings[:, 0] = (node_attributes_a[-1])[dist_filter]\n",
    "\n",
    "dist_to_point_mapping = calc_point_indices_to_distributions(distributions, points)\n",
    "\n",
    "for i, dest_index in enumerate(dist_to_point_mapping):\n",
    "    if dest_index == -1: continue\n",
    "    relative_point_index = indicies_of_filterted_array_entries(points_filter, i)\n",
    "    mappings[dest_index, 1] = (node_attributes_b[-1])[relative_point_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70159d14-ff46-4723-83c1-6384691f30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8366ad3-dcd4-4d9c-9710-23133b48f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d0e9a-6e49-4d3a-add3-670fa560dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(dist_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28234a1-14e6-42fc-b11a-888a3ca90391",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_point_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83b223-0a74-47cc-9a23-ed4390a1bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbc008-cd3c-41ad-b939-7ac4336215d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_point_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056bf28-38ca-41ee-a907-defc1d8be707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba6fcf-f7f6-49ba-a94e-84bb13441f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_indicies.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104734f1-cb7e-4c4f-a891-1b62f65bb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies_of_filterted_array_entries(points_filter, calc_point_indices_to_distributions(distributions, points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8090745-0066-46ce-b676-42304b126f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_nodes = torch.zeros((distributions.shape[0], 2)) - 1\n",
    "matching_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb54ec-19db-480a-af99-1948ad1b8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = np.array([\n",
    "    [[3, 0], [1, 0], [0, 1]],\n",
    "    [[2, 0], [1, 0.5], [0.5, 1]]\n",
    "])\n",
    "\n",
    "points = np.random.uniform(-10, 10, (2, 2))\n",
    "\n",
    "# distributions[:, 0, :] += points\n",
    "# distributions\n",
    "\n",
    "# calc_likelihoods_for_distributions_and_points(distributions, points)\n",
    "calc_point_indices_to_distributions(distributions, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16a1e7-7db6-4ab5-a4e9-933cf6c1a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_point_indices_to_distributions(distributions, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6cae1-2ff7-4e5b-a4a8-332be0a81921",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attributes_b[0] == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f8410-3a89-43e2-be0f-e5d1173db11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(node_attributes_b[0] == label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fec71e-d362-4ac8-bbc2-837795e4ce81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67c12a-6fea-474f-ad7e-540513d87ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes_dict = {\n",
    "#     'goomba': 0,\n",
    "#     'mario': 1,\n",
    "#     'cloud': 2,\n",
    "#     'ground': 3,\n",
    "#     'bush': 4,\n",
    "#     'box': 5,\n",
    "#     'pipe': 6\n",
    "# }\n",
    "\n",
    "# class_names, boxes = get_classnames_boxes_from_csv(csv_file_name, iteration)\n",
    "# num_nodes = len(boxes)\n",
    "# edge_connections = cartesian_product_for_nodes(range(num_nodes))\n",
    "# node_features = []\n",
    "# matrix = np.empty((0, 2))\n",
    "# normal_dist = [0, 0, 1, 0, 1, 0] # mu1, mu2, sig00, sig01, sig10, sig11\n",
    "\n",
    "# if num_nodes == 1:\n",
    "#     box = boxes[0]\n",
    "#     width, height = abs(box['left'] - box['right']), abs(box['top'] - box['bottom'])\n",
    "#     node_features.append((*normal_dist, classes_dict[class_names[0]], width, height))\n",
    "    \n",
    "# for i, box in enumerate(boxes):\n",
    "#     new_row = np.array([[box['center_x'], box['center_y']]])\n",
    "#     matrix = np.vstack((matrix, new_row))\n",
    "\n",
    "#     width, height = abs(box['left'] - box['right']), abs(box['top'] - box['bottom'])\n",
    "#     node_features.append((*normal_dist, classes_dict[class_names[i]], width, height))\n",
    "\n",
    "# dists, angles = dist_angle_from_matrix(matrix, edge_connections)\n",
    "\n",
    "# node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "# edge_connections = torch.tensor(edge_connections)\n",
    "# edges_features =  torch.tensor(np.stack((dists, angles), axis=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
