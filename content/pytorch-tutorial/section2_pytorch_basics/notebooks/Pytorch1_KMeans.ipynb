{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Kmeans Clustering</h1>\n",
    "<img src=\"../data/K_means.gif\" width=\"1000\" align=\"center\">\n",
    "With our knowledge of Python and now Numpy lets create an implementation of a famous machine learning algorithm \"K-Means Clustering\". The job of a clustering algorithm is to break a dataset into some number of \"clusters\" (groups), the number of clusters usually defined by the user. K-Means clustering working by iteratively updating  a pre-defined number of cluster centers. It does this by finding the distance between each datapoint and every cluster center. Datapoints are then assigned to the cluster center they are closest to and each cluster center is updated to be the mean of the new cluster. These steps are updated for some number of steps or until the cluster centers converge (they stop moving so much)<br>\n",
    "<b>Lets have a look at the steps of K-means clustering</b><br>\n",
    "1. Define the number of clusters \"k\" you want to group your data into<br>\n",
    "2. Randomly initialise k vectors with the same size as each datapoint, this is the initialisation of our cluster centers<br>\n",
    "3. Calculate the distance between each datapoint and each cluster center (using MSE or equivalent)<br>\n",
    "4. For every datapoint find the cluster center they are closest to<br>\n",
    "5. Re-calculate the cluster centers by finding the mean of every new cluster<br>\n",
    "6. Repeat steps 3-6 for n steps or until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib\n",
    "%pip install -q torch\n",
    "%pip install -q numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../datasets/MNIST/raw/train-images-idx3-ubyte.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m     \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_x\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_output\n",
      "File \u001b[0;32m/workspaces/supermario-rl/pytorch-tutorial/section2_pytorch_basics/notebooks/load.py:124\u001b[0m\n\u001b[1;32m    121\u001b[0m img_dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m    122\u001b[0m img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m784\u001b[39m\n\u001b[0;32m--> 124\u001b[0m (train_x, train_y), (test_x, test_y) \u001b[38;5;241m=\u001b[39m \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/supermario-rl/pytorch-tutorial/section2_pytorch_basics/notebooks/load.py:82\u001b[0m, in \u001b[0;36mload_mnist\u001b[0;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m(Trainig Image, Training Label), (Test Image, Test Label)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_file):\n\u001b[0;32m---> 82\u001b[0m     \u001b[43minit_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     85\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m/workspaces/supermario-rl/pytorch-tutorial/section2_pytorch_basics/notebooks/load.py:55\u001b[0m, in \u001b[0;36minit_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_mnist\u001b[39m():\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mdownload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _convert_numpy()\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating pickle file ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/supermario-rl/pytorch-tutorial/section2_pytorch_basics/notebooks/load.py:22\u001b[0m, in \u001b[0;36mdownload_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_mnist\u001b[39m():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m key_file\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 22\u001b[0m         \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/supermario-rl/pytorch-tutorial/section2_pytorch_basics/notebooks/load.py:17\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_base\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/urllib/request.py:257\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Handle temporary file setup.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m--> 257\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../datasets/MNIST/raw/train-images-idx3-ubyte.gz'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np     \n",
    "import torch\n",
    "from load import test_x\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Using the module \"load\" that comes with this notebook, lets load our dataset</b><br>\n",
    "The dataset we'll be using is the MNIST dataset, a dataset of small, low-res handwritten digits. There are 60000 training images and 10000 test images divided up into 10 classes (digits 0-9). Here we will be using the test set (as it's a smaller set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of datapoint\n",
    "num_img = 10000  \n",
    "#Number of cluster centers, 10 because the dataset contains 10 classes eg: digit 0 to 9\n",
    "num_means = 10   \n",
    "#We'll perform this many iterations of the algorithm\n",
    "iterations = 20 \n",
    "#Each image is 28*28 pixels, which has been flattened to a vector 0f 784 values\n",
    "data_size = 28*28\n",
    "# The images are 8 bit greyscale images (values range from 0-255)\n",
    "# We'll rescale the pixel values to be between 0-1 (We don't REALLY need to do this for k-means)\n",
    "test_x_tensor = torch.FloatTensor((test_x.astype(float) / 255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Kmeans Initialization </h3>\n",
    "Here we'll initialise the cluster centers to random values by creating a 10*784 matrix (2D Tensor) by randomly sampling 10 points from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generate K indicies for k datapoints from the dataset (indicies need to be int)\n",
    "means  = test_x_tensor[np.random.randint(0, num_img , num_means)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Kmeans Algorithm </h3>\n",
    "Now implement the main steps of the K-Means clustering algorithm! Try and make it as efficient as possible and minimise the time/iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "\n",
    "    print(\"Itteration [%d/%d]\" % (i,iterations))\n",
    "end_time = time.time()\n",
    "print(\"%d iterations took %.2f seconds, which corresponds to %.2fs/iteration\" % (iterations, end_time - start_time, (end_time - start_time)/iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets visualise the the cluster centers!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(20, 10))\n",
    "img = means.cpu().view(num_means*28,28)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
