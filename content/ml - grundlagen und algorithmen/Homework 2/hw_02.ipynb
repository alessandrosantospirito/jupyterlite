{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2 - ML - Grundverfahren SS 23\n",
    "\n",
    "Ge Li ge.li@kit.edu. This homework has 4 topics with 24 points in total.\n",
    "\n",
    "## Submission Instructions\n",
    "Submission deadline: May 31, 2023, 15:00 Berlin time. Please follow the instruction from homework ZERO!\n",
    "\n",
    "\n",
    "## 1.) Probability Basics and Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Example (Two Moons)\n",
    "\n",
    "Let us start by loading a very simple toy dataset, the \"two moons\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "data = dict(np.load(\"two_moons.npz\", allow_pickle=True))\n",
    "samples = data[\"samples\"]\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "c0_samples = samples[labels == 0]  # class 0: all samples with label 0\n",
    "c1_samples = samples[labels == 1]  # class 1: all samples with labe 1\n",
    "\n",
    "plt.figure(\"Data\")\n",
    "plt.scatter(x=c0_samples[:, 0], y=c0_samples[:, 1], label=\"c0\")\n",
    "plt.scatter(x=c1_samples[:, 0], y=c1_samples[:, 1], label=\"c1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also define some plotting utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2d_gaussian(mu: np.ndarray, sigma: np.ndarray, plt_std: float = 2, *args, **kwargs) -> None:\n",
    "    (largest_eigval, smallest_eigval), eigvec = np.linalg.eig(sigma)\n",
    "    phi = -np.arctan2(eigvec[0, 1], eigvec[0, 0])\n",
    "\n",
    "    plt.scatter(mu[0:1], mu[1:2], marker=\"x\", *args, **kwargs)\n",
    "\n",
    "    a = plt_std * np.sqrt(largest_eigval)\n",
    "    b = plt_std * np.sqrt(smallest_eigval)\n",
    "\n",
    "    ellipse_x_r = a * np.cos(np.linspace(0, 2 * np.pi, num=200))\n",
    "    ellipse_y_r = b * np.sin(np.linspace(0, 2 * np.pi, num=200))\n",
    "\n",
    "    R = np.array([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]])\n",
    "    r_ellipse = np.array([ellipse_x_r, ellipse_y_r]).T @ R\n",
    "    plt.plot(mu[0] + r_ellipse[:, 0], mu[1] + r_ellipse[:, 1], *args, **kwargs)\n",
    "\n",
    "# plot grid for contour plots\n",
    "plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using Generative Models (Naive Bayes Classifier)\n",
    "\n",
    "We first try a generative approach, the Naive Bayes Classifier.\n",
    "We model the class conditional distributions $p(\\boldsymbol{x}|c)$ as Gaussians, the class prior $p(c)$ as\n",
    "Bernoulli and apply Bayes rule to compute the class posterior $p(c|\\boldsymbol{x})$.\n",
    "\n",
    "As a small recap, recall that the density of the Multivariate Normal Distribution is given by\n",
    "\n",
    "$$ p(\\boldsymbol{x}) = \\mathcal{N}\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\right) = \\dfrac{1}{\\sqrt{\\det \\left(2 \\pi \\boldsymbol{\\Sigma}\\right)}} \\exp\\left( - \\dfrac{(\\boldsymbol{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x}-\\boldsymbol{\\mu})}{2}\\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvn_pdf(x: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Density of the Multivariate Normal Distribution\n",
    "    :param x: samples, shape: [N x dimension]\n",
    "    :param mu: mean, shape: [dimension]\n",
    "    :param sigma: covariance, shape: [dimension x dimension]\n",
    "    :return p(x) with p(x) = N(mu, sigma) , shape: [N]\n",
    "    \"\"\"\n",
    "    norm_term = 1 / np.sqrt(np.linalg.det(2 * np.pi * sigma))\n",
    "    diff = x - np.atleast_2d(mu)\n",
    "    exp_term = np.sum(np.linalg.solve(sigma, diff.T).T * diff, axis=-1)\n",
    "    return norm_term * np.exp(-0.5 * exp_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Aspect:** In practice you would never implement it like that, but stay\n",
    "in the log-domain. Also for numerically stable implementations of the multivariate normal density the symmetry and\n",
    "positive definitness of the covariance should be exploited by working with it's Cholesky decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimator for a Multivariate Normal Distribution is given by\n",
    "$$ \\boldsymbol{\\mu} = \\dfrac{1}{N} \\sum_{i}^N \\boldsymbol{x}_i \\quad \\quad \\boldsymbol{\\Sigma} = \\dfrac{1}{N} \\sum_{i}^N (\\boldsymbol{x}_i - \\boldsymbol{\\mu}) (\\boldsymbol{x}_i - \\boldsymbol{\\mu})^T. $$\n",
    "\n",
    "This time, before we use it, we are going to derive it:\n",
    "\n",
    "### Exercise 1.1): Derivation of Maximum Likelihood Estimator (5 Points):\n",
    "\n",
    "Derive the maximum likelihood estimator for Multivariate Normal distributions, given above.\n",
    "This derivations involves some matrix calculus.\n",
    "Matrix calculus is a bit like programming, you google the stuff you need and then plug it together in the right order.\n",
    "Good resources for such rules are the \"matrix cookbook\" (https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) and the Wikipdia article about matrix calculus\n",
    "(https://en.wikipedia.org/wiki/Matrix_calculus ). State all rules you use explicitly\n",
    "(except the ones given in the hints below).\n",
    "\n",
    "**Remark** There are different conventions of how to define a gradient (as column-vector or row-vector). This results in different ways to write the Jacobian and thus different, usually transposed, matrix calculus rules:\n",
    "- In the lecture we define the gradient as column-vector\n",
    "- In the Wikipedia article this convention is referred to as \"Denominator Layout\". It also contains a nice explanation of the different conventions for the gourmets among you ;)\n",
    "- The Matrix Cookbook uses the same convention (gradient as column vector)\n",
    "- Please also use it here\n",
    "\n",
    "**Hint** Here are two of those rules that might come in handy\n",
    "\n",
    "$\\dfrac{\\partial\\log\\det(\\boldsymbol{X})}{\\partial \\boldsymbol{X}} = \\boldsymbol{X}^{-1}$\n",
    "\n",
    "$\\dfrac{\\partial \\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{x}}{\\partial \\boldsymbol{x}} = 2 \\boldsymbol{A}\\boldsymbol{x}$ for symmetric matrices $\\boldsymbol{A}$ (Hint hint: covariance matrices are always\n",
    "symmetric)\n",
    "\n",
    "There is one missing to solve the exercise. You need to find it yourself. (Hint hint: Look in the matrix cookbook, chapter 2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "We use the final results from the derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvn_mle(x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Estimation of parameters for Multivariate Normal Distribution\n",
    "    :param x: samples shape: [N x dimension]\n",
    "    :return mean (shape: [dimension]) und covariance (shape: [dimension x dimension]) that maximize likelihood of data.\n",
    "    \"\"\"\n",
    "    mean = 1 / x.shape[0] * np.sum(x, axis=0)\n",
    "    diff = x - mean\n",
    "    cov = 1 / x.shape[0] * diff.T @ diff\n",
    "    return mean, cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this maximum likelihood estimator to fit generative models to the samples of both classes. Using those models and some basic rules of probability we can obtain the class conditional distribution $p(c|\\boldsymbol{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2) Generative Classifier (2 Points)\n",
    "\n",
    "Given a way to fit the class conditional using our Maximum Likelihood estimator, we can implement the generative classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gaussian Distributions using the maximum likelihood estimator to samples from both classes\n",
    "mu_c0, sigma_c0 = mvn_mle(c0_samples)\n",
    "mu_c1, sigma_c1 = mvn_mle(c1_samples)\n",
    "\n",
    "# Prior obtained by \"counting\" samples in each class\n",
    "p_c0 = c0_samples.shape[0] / samples.shape[0]\n",
    "# LEAVE AS EXERCISE\n",
    "p_c1 = c1_samples.shape[0] / samples.shape[0]  # = 1 - p_c0\n",
    "\n",
    "def compute_posterior(\n",
    "        samples: np.ndarray,\n",
    "        p_c0: float, mu_c0: np.ndarray, sigma_c0: np.ndarray,\n",
    "        p_c1: float, mu_c1: np.ndarray, sigma_c1: np.ndarray) \\\n",
    "        -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    computes the posteroir distribution p(c|x) given samples x, the prior p(c) and the\n",
    "    class conditional likelihood p(x|c)\n",
    "    :param samples: samples x to classify, shape: [N x dimension]\n",
    "    :param p_c0: prior probability of class 0, p(c=0)\n",
    "    :param mu_c0: mean of class conditional likelihood of class 0, p(x|c=0) shape: [dimension]\n",
    "    :param sigma_c0: covariance of class conditional likelihood of class 0, p(x|c=0) shape: [dimension x dimension]\n",
    "    :param p_c1: prior probability of class 1 p(c=1)\n",
    "    :param mu_c1: mean of class conditional likelihood of class 1 p(x|c=1) shape: [dimension]\n",
    "    :param sigma_c1: covariance of class conditional likelihood of class 1, p(x|c=1) shape: [dimension x dimension]\n",
    "    :return two arrays, p(c=0|x) and p(c=1|x), both shape [N]\n",
    "    \"\"\"\n",
    "    # TODO: compute class likelihoods\n",
    "\n",
    "\n",
    "    # TODO: compute normalization using marginalization\n",
    "\n",
    "\n",
    "    # TODO: compute class posterior using Bayes rule\n",
    "\n",
    "    \n",
    "    \n",
    "    return p_c0_given_x, 1 - p_c0_given_x\n",
    "\n",
    "\n",
    "p_c0_given_x, p_c1_given_x = compute_posterior(samples, p_c0, mu_c0, sigma_c0, p_c1, mu_c1, sigma_c1)\n",
    "# Prediction\n",
    "predicted_labels = np.zeros(labels.shape)\n",
    "# break at 0.5 arbitrary\n",
    "predicted_labels[p_c0_given_x >= 0.5] = 0.0  # is not strictly necessary since whole array already zero.\n",
    "predicted_labels[p_c1_given_x > 0.5] = 1.0\n",
    "\n",
    "# Evaluate\n",
    "acc = (np.count_nonzero(predicted_labels == labels)) / labels.shape[0]\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the class likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Naive Bayes\")\n",
    "plt.scatter(x=samples[labels == 0, 0], y=samples[labels == 0, 1], c=\"blue\")\n",
    "draw_2d_gaussian(mu_c0, sigma_c0, c=\"blue\")\n",
    "plt.scatter(x=samples[labels == 1, 0], y=samples[labels == 1, 1], c=\"orange\")\n",
    "draw_2d_gaussian(mu_c1, sigma_c1, c=\"orange\")\n",
    "plt.legend([\"c0\", \"c1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final posterior distribution for the case $p(c=1|\\boldsymbol{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c0_given_x, p_c1_given_x = compute_posterior(flat_plt_grid, p_c0, mu_c0, sigma_c0, p_c1, mu_c1, sigma_c1)\n",
    "p_c0_given_x = np.reshape(p_c0_given_x, plt_grid_shape)\n",
    "p_c1_given_x = np.reshape(p_c1_given_x, plt_grid_shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], p_c0_given_x, levels=10)\n",
    "plt.colorbar()\n",
    "# plot decision boundary\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], p_c0_given_x, levels=[0.0, 0.5], colors=[\"k\", \"k\"])\n",
    "\n",
    "plt.title(\"p($c_0$ | x)\")\n",
    "s0 = plt.scatter(c0_samples[..., 0], c0_samples[..., 1], color=\"blue\")\n",
    "s1 = plt.scatter(c1_samples[..., 0], c1_samples[..., 1], color=\"orange\")\n",
    "plt.legend([s0, s1], [\"c0\", \"c1\"])\n",
    "plt.xlim(-1.5, 2.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], p_c1_given_x, levels=10)\n",
    "plt.colorbar()\n",
    "# plot decision boundary\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], p_c0_given_x, levels=[0.0, 0.5], colors=[\"k\", \"k\"])\n",
    "plt.title(\"p($c_1$ | x)\")\n",
    "s0 = plt.scatter(c0_samples[..., 0], c0_samples[..., 1], color=\"blue\")\n",
    "s1 = plt.scatter(c1_samples[..., 0], c1_samples[..., 1], color=\"orange\")\n",
    "plt.legend([s0, s1], [\"c0\", \"c1\"])\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color indicates the posterior likelihood for the respective call and the black line indicates the decision boundary.\n",
    "We achieve a train accuracy of 87%.\n",
    "For such a simple task that is clearly not great, but it nicely illustrates a\n",
    "problem with generative approaches:\n",
    "They usually depend on quite a lot of assumptions.\n",
    "\n",
    "### Exercise 1.3) Wrong Assumptions? (1 Point):\n",
    "Which untrue assumption did we make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Stochastic and Batch Gradients\n",
    "\n",
    "In the recap sessions with Prof. Neumann we already saw (or will see) an implementation of a Discriminative Classifier using Logistic Regression. Here we are going to extend this to stochastic and batch gradient descent.\n",
    "\n",
    "We start by implementing a few helper functions for affine mappings, the sigmoid function, and the negative Bernoulli log-likelihood. - Those are the same as used for the full gradient case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements affine feature function\n",
    "    :param x: inputs, shape: [N x sample_dim]\n",
    "    :return inputs with additional bias dimension, shape: [N x feature_dim]\n",
    "    \"\"\"\n",
    "    return np.concatenate([x, np.ones((x.shape[0], 1))], axis=-1)\n",
    "\n",
    "def quad_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements quadratic feature function\n",
    "    :param x: inputs, shape: [N x sample_dim]\n",
    "    :return squared features of x, shape: [N x feature_dim]\n",
    "    \"\"\"\n",
    "    sq = np.stack([x[:, 0] ** 2, x[:, 1]**2, x[:, 0] * x[:, 1]], axis=-1)\n",
    "    return np.concatenate([sq, affine_features(x)], axis=-1)\n",
    "\n",
    "def cubic_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements cubic feature function\n",
    "    :param x: inputs, shape: [N x sample_dim]\n",
    "    :return cubic features of x, shape: [N x feature_dim]\n",
    "    \"\"\"\n",
    "    cubic = np.stack([x[:, 0]**3, x[:, 0]**2 * x[:, 1], x[:, 0] * x[:, 1]**2, x[:, 1]**3], axis=-1)\n",
    "    return np.concatenate([cubic, quad_features(x)], axis=-1)\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the sigmoid function\n",
    "    :param x: inputs\n",
    "    :return sigma(x)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def bernoulli_nll(predictions: np.ndarray, labels: np.ndarray, epsilon: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param predictions: output of the classifier, shape: [N]\n",
    "    :param labels: true labels of the samples, shape: [N]\n",
    "    :param epsilon: small offset to avoid numerical instabilities (i.e log(0))\n",
    "    :return negative log-likelihood of the labels given the predictions\n",
    "    \"\"\"\n",
    "    return - (labels * np.log(predictions + epsilon) + (1 - labels) * np.log(1 - predictions + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also using the same bernoulli objective and its gradient as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_bern(weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    bernoulli log-likelihood objective\n",
    "    :param weights: current weights to evaluate, shape: [feature_dim]\n",
    "    :param features: train samples, shape: [N x feature_dim]\n",
    "    :param labels: class labels corresponding to train samples, shape: [N]\n",
    "    :return average negative log-likelihood\n",
    "    \"\"\"\n",
    "    predictions = sigmoid(features @ weights)\n",
    "    return np.mean(bernoulli_nll(predictions, labels))\n",
    "\n",
    "def d_objective_bern(weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of the bernoulli log-likelihood objective\n",
    "    :param weights: current weights to evaluate, shape: [feature_dim]\n",
    "    :param features: train samples, shape: [N x feature_dim]\n",
    "    :param labels: class labels corresponding to train samples, shape [N]\n",
    "    \"\"\"\n",
    "    res = np.expand_dims(sigmoid(features @ weights) - labels, -1)\n",
    "    grad = features.T @ res / res.shape[0]\n",
    "    return np.squeeze(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1) Implementation (3 Points)\n",
    "\n",
    "Finally, we can implement our batch gradient descent optimizer. When setting the batch_size to 1 it will become a stochastic gradient descent optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_with_sgd(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, schedule: Callable,\n",
    "                      num_iterations: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    :param features: all samples, shape: [N x feature_dim]\n",
    "    :param labels: all labels, shape: [N]\n",
    "    :param initial_weights: initial weights of the classifier, shape: [feature_dim * K]\n",
    "    :param schedule: learning rate schedule (a callable function returning the learning rate, given the iteration\n",
    "    :param num_iterations: number of times to loop over the whole dataset\n",
    "    :param batch_size: size of each batch, should be between 1 and size of data\n",
    "    return \"argmin\", \"min\", logging info\n",
    "    \"\"\"\n",
    "\n",
    "    assert 1 <= batch_size <= features.shape[0]\n",
    "    # This is a somewhat simplifying assumption but for the exercise its ok\n",
    "    assert features.shape[0] % batch_size == 0, \"Batch Size does not evenly divide number of samples\"\n",
    "    batches_per_iter = int(features.shape[0] / batch_size)\n",
    "\n",
    "    # setup\n",
    "    weights = np.zeros([batches_per_iter * num_iterations + 1, initial_weights.shape[0]])\n",
    "    loss = np.zeros(batches_per_iter * num_iterations + 1)\n",
    "    weights[0] = initial_weights\n",
    "    loss[0]= objective_bern(weights[0], features, labels)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        #--------------------------------------------------\n",
    "        # TODO: shuffle data / get random indices\n",
    "        #--------------------------------------------------\n",
    "        # sample random batches for current iteration\n",
    "\n",
    "        \n",
    "        for j in range(batches_per_iter):\n",
    "\n",
    "            global_idx = i * batches_per_iter + j\n",
    "\n",
    "            #--------------------------------------------------\n",
    "            # TODO: do stochastic gradient descent update!\n",
    "            #--------------------------------------------------\n",
    "\n",
    "            # get current batches\n",
    "            # TODO\n",
    "\n",
    "            # get learning rate and perform normal gradient descent step\n",
    "            # TODO\n",
    "\n",
    "            # log loss (on all samples, usually you should not use all samples to evaluate after each stochastic\n",
    "            # update step)\n",
    "            loss[global_idx + 1] = objective_bern(weights[global_idx + 1], features, labels)\n",
    "    return weights[-1], loss[-1], (weights, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve is expected to look a bit jerky due to the stochastic nature of stochastic gradient descent.\n",
    "If it goes down asymptotically its fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Features from Data\n",
    "\n",
    "# change this to play arround with feature functions\n",
    "#feature_fn = affine_features\n",
    "#feature_fn = quad_features\n",
    "feature_fn = cubic_features\n",
    "features = feature_fn(samples)\n",
    "\n",
    "num_iterations = 125\n",
    "\n",
    "w_bce, l, l_info = minimize_with_sgd(features, labels, np.zeros(features.shape[1]),\n",
    "                                 schedule=(lambda t: 0.25),\n",
    "                                 num_iterations=num_iterations,\n",
    "                                 batch_size=1)\n",
    "print(\"Final loss\", l)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Cross Entropy Loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.xlabel(\"Update Steps\")\n",
    "plt.ylabel(\"Negative Bernoulli Log-Likelihood\")\n",
    "plt.semilogy(l_info[1])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Bernoulli LL Solution\")\n",
    "pred_grid = np.reshape(sigmoid(feature_fn(flat_plt_grid) @ w_bce), plt_grid_shape)\n",
    "\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "plt.colorbar()\n",
    "#This is just a very hacky way to get a black line at the decision boundary:\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[0, 0.5], colors=[\"k\"])\n",
    "\n",
    "s0 = plt.scatter(c0_samples[..., 0], c0_samples[..., 1], color=\"blue\")\n",
    "s1 = plt.scatter(c1_samples[..., 0], c1_samples[..., 1], color=\"orange\")\n",
    "plt.legend([s0, s1], [\"c0\", \"c1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2) Effect of different Batch Sizes and Number of Iterations (1. Point)\n",
    "Play around with the batch size and number of iterations and briefly describe your observations about convergence speed and monotonicity  of the loss curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Multiclass Classification\n",
    "\n",
    "The Iris Dataset is a very classical machine learning and statistics benchmark for classification, developed in the 1930's. The goal is to classify 3 types of flowers (more specifically, 3 types of flowers form the Iris species) based on 4 features: petal length, petal width, sepal length and sepal width.\n",
    "\n",
    "As we have $K=3$ different types of flowers we are dealing with a multi-class classification problem and need to extend our sigmoid-based classifier from the previous exercise / recap session.\n",
    "\n",
    "We will reuse our \"minimize\" and \"affine feature\" functions. Those are exactly as before. The affine features are sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def minimize(f: Callable , df: Callable, x0: np.ndarray, lr: float, num_iters: int) -> \\\n",
    "        Tuple[np.ndarray, float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param f: objective function\n",
    "    :param df: gradient of objective function\n",
    "    :param x0: start point, shape [dimension]\n",
    "    :param lr: learning rate\n",
    "    :param num_iters: maximum number of iterations\n",
    "    :return argmin, min, values of x for all interations, value of f(x) for all iterations\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    x = np.zeros([num_iters + 1] + list(x0.shape))\n",
    "    f_x = np.zeros(num_iters + 1)\n",
    "    x[0] = x0\n",
    "    f_x[0] = f(x0)\n",
    "    for i in range(num_iters):\n",
    "        # update using gradient descent rule\n",
    "        grad = df(x[i])\n",
    "        x[i + 1] = x[i] - lr * grad\n",
    "        f_x[i + 1] = f(x[i + 1])\n",
    "    return x[i+1], f_x[i+1], x[:i+1], f_x[:i+1] # logging info for visualization\n",
    "\n",
    "\n",
    "def affine_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements affine feature function\n",
    "    :param x: inputs\n",
    "    :return inputs with additional bias dimension\n",
    "    \"\"\"\n",
    "    return np.concatenate([x, np.ones((x.shape[0], 1))], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "In the original dataset the different types of flowers are labeled with $0, 1$ and $2$. The output of our classifier will be a vector with $K=3$ entries, $\\begin{pmatrix}p(c=0 | \\boldsymbol x) & p(c=1 | \\boldsymbol x) & p(c=2 | \\boldsymbol x) \\end{pmatrix}$, i.e. the probability for each class that a given sample is an instance of that class, given a datapoint $\\boldsymbol x$. As presented in the lecture, working with categorical (=multinomial) distributions is easiest when we represent the labels in a different form, a so called one-hot encoding. This is a vector of the length of number of classes, in this case 3, with zeros everywhere except for the entry corresponding to the class number, which is one. For the train and test data we know to which class it belongs, so the probability for that class is one and the probability for all other classes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"iris_data.npz\")\n",
    "train_samples = data[\"train_features\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "test_samples = data[\"test_features\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "\n",
    "train_features = affine_features(train_samples)\n",
    "test_features = affine_features(test_samples)\n",
    "\n",
    "def generate_one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param y: vector containing classes as numbers, shape: [N]\n",
    "    :param num_classes: number of classes\n",
    "    :return a matrix containing the labels in an one-hot encoding, shape: [N x K]\n",
    "    \"\"\"\n",
    "    y_oh = np.zeros([y.shape[0], num_classes])\n",
    "\n",
    "    # can be done more efficiently using numpy with\n",
    "    # y_oh[np.arange(y.size), y] = 1.0\n",
    "    # we use the for loop for clarity\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        y_oh[i, y[i]] = 1.0\n",
    "\n",
    "    return y_oh\n",
    "\n",
    "\n",
    "oh_train_labels = generate_one_hot_encoding(train_labels, 3)\n",
    "oh_test_labels = generate_one_hot_encoding(test_labels, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using Gradient Descent\n",
    "\n",
    "The multi-class generalization of the sigmoid is the softmax function. It takes an vector of length $K$ and outputs another vector of length $K$ where the $k$-th entry is given by\n",
    "$$ \\textrm{softmax}(\\boldsymbol{x})_k = \\dfrac{\\exp(x_k)}{\\sum_{j=1}^K \\exp(x_j)}.$$\n",
    "This vector contains positive elements which sum to $1$ and thus can be interpreted as parameters of a categorical distribution. Lets see how we can implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"softmax function\n",
    "    :param x: inputs, shape: [N x K]\n",
    "    :return softmax(x), shape [N x K]\n",
    "    \"\"\"\n",
    "    a = np.max(x, axis=-1, keepdims=True)\n",
    "    log_normalizer = a + np.log(np.sum(np.exp(x - a), axis=-1, keepdims=True))\n",
    "    return np.exp(x - log_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Aspect:** In the above implementation of the softmax we stayed in the log-domain until the very last command.\n",
    "We also used the log-sum-exp-trick (https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations).\n",
    "Staying in the log domain and applying the log-sum-exp-trick whenever possible is a simple way to make the implementation\n",
    "numerically more robust. It does not change anything with regards to the underlying theory.\n",
    "\n",
    "We also need to extend our loss function. Instead of the log-likelihood of a Bernoulli distribution, we now maximize the log-likelihood of a categorical distribution which, for a single sample $\\boldsymbol{x}_i$, is given by\n",
    "$$\\log p(c_i | \\boldsymbol x_i) = \\sum_{k=1}^K h_{i, k} \\log(p_{i,k})$$\n",
    "where $\\boldsymbol h_i$ denotes the one-hot encoded true label and $p_{i,k} \\equiv p(c_i = k | \\boldsymbol x_i)$ the class probabilities predicted by the classifier. In multiclass classification, we learn one weight vector $\\boldsymbol w_k$ per class s.t. those probabilities are given by $p(c_i = k | \\boldsymbol x_i) = \\mathrm{softmax}(\\boldsymbol w^T \\boldsymbol \\phi (\\boldsymbol x_i))_k $.\n",
    "We can now implement the (negative) log-likelihood of a categorical distribution (we use the negative log-likelihood as we will minimize the loss later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_nll(predictions: np.ndarray, labels: np.ndarray, epsilon: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    cross entropy loss function\n",
    "    :param predictions: class labels predicted by the classifier, shape: [N x K]\n",
    "    :param labels: true class labels, shape: [N x K]\n",
    "    :param epsilon: small offset to avoid numerical instabilities (i.e log(0))\n",
    "    :return negative log-likelihood of the labels given the predictions, shape: [N]\n",
    "    \"\"\"\n",
    "    return - np.sum(labels * np.log(predictions + epsilon), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the loss for a single sample. To get the loss for all samples we will need to sum over loss for a single sample\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L_{\\mathrm{cat-NLL}}\n",
    "=& - \\sum_{i=1}^N \\log p(c_i | \\boldsymbol x_i) \\\\\n",
    "=& - \\sum_{i=1}^N \\sum_{k=1}^K h_{i, k} \\log(p_{i,k}) \\\\\n",
    "=& - \\sum_{i=1}^N  \\sum_{k=1}^K h_{i, k} \\log(\\textrm{softmax}(\\boldsymbol{w}_k^T \\boldsymbol \\phi(\\boldsymbol{x}_i))_k)\\\\\n",
    "=& - \\sum_{i=1}^N \\left(\\sum_{k=1}^K h_{i,k}\\boldsymbol{w}^T_k \\boldsymbol \\phi(\\boldsymbol{x}_i) - \\log \\sum_{j=1}^K \\exp(\\boldsymbol{w}_j^T \\boldsymbol \\phi(\\boldsymbol{x}_i))\\right).\n",
    "\\end{align}\n",
    "\n",
    "In order to use gradient based optimization for this, we of course also need to derive the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1) Derivation (4 Points)\n",
    "Derive the gradient $\\dfrac{\\partial \\mathcal L_{\\mathrm{cat-NLL}}}{\\partial \\boldsymbol{w}}$ of the loss function w.r.t. the full weight vector $\\boldsymbol w \\equiv \\begin{pmatrix} \\boldsymbol w_1^T & \\dots & \\boldsymbol w_K^T \\end{pmatrix}^T$, which is obtained by stacking the class-specific weight vectors $\\boldsymbol w_k$.\n",
    "\n",
    "**Hint 1:** Follow the steps in the derivation of the gradient of the loss for the binary classification in the lecture.\n",
    "\n",
    "**Hint 2:** Derive the gradient not for the whole vector $\\boldsymbol w$ but only for $\\boldsymbol w_k$, i.e., $\\dfrac{\\partial \\mathcal L_{\\mathrm{cat-NLL}}}{\\partial \\boldsymbol{w}_k}$. The gradients for the individual\n",
    "$\\boldsymbol w_k$ can then be stacked to obtain the full gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2) Implementation (3 Points)\n",
    "Now that we have the formulas for the loss and its gradient, we can implement them. Fill in the function skeletons below so that they implement the loss and its gradient. Again, in praxis, it is advisable to work with the mean nll instead of the sum, as this simplifies setting the learning rate.\n",
    "\n",
    "**Hint:** The optimizer works with vectors only. So the function get the weights as vectors in the flat_weights parameter.\n",
    "Make sure you use efficient vectorized computations (no for-loops!). Thus, we reshape the weights appropriately before using them for the computations. For the gradients make sure to return\n",
    "again a vector by flattening the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective\n",
    "def objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N x K]\n",
    "    :return cross entropy loss of the classifier given the samples\n",
    "    \"\"\"\n",
    "    num_features = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [num_features, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# derivative\n",
    "def d_objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N x K]\n",
    "    :return gradient of cross entropy loss of the classifier given the samples, shape: [feature_dim * K]\n",
    "    \"\"\"\n",
    "    feature_dim = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [feature_dim, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO, do not forget to flatten the gradient before returning!\n",
    "    #---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tie everything together again. Both train and test accuracy should be at least 0.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# optimization\n",
    "\n",
    "w0_flat = np.zeros(5 * 3) # 4 features + bias, 3 classes\n",
    "w_opt_flat, loss_opt, x_history, f_x_history = \\\n",
    "   minimize(lambda w: objective_cat(w, train_features, oh_train_labels),\n",
    "            lambda w: d_objective_cat(w, train_features, oh_train_labels),\n",
    "            w0_flat, 1e-2, 1000)\n",
    "\n",
    "w_opt = np.reshape(w_opt_flat, [5, 3])\n",
    "\n",
    "# plotting and evaluation\n",
    "print(\"Final Loss:\", loss_opt)\n",
    "plt.figure()\n",
    "plt.plot(f_x_history)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"negative categorical log-likelihood\")\n",
    "\n",
    "train_pred = softmax(train_features @ w_opt)\n",
    "train_acc = np.count_nonzero(np.argmax(train_pred, axis=-1) == np.argmax(oh_train_labels, axis=-1))\n",
    "train_acc /= train_labels.shape[0]\n",
    "test_pred = softmax(test_features @ w_opt)\n",
    "test_acc = np.count_nonzero(np.argmax(test_pred, axis=-1) == np.argmax(oh_test_labels, axis=-1))\n",
    "test_acc /= test_labels.shape[0]\n",
    "print(\"Train Accuracy:\", train_acc, \"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Feature-Based Support Vector Machine (Hinge Loss) (5 Points)\n",
    "In this exercise, we will train a feature-based SVM on the two moons dataset using the hinge loss. We will use the L-BFGS-B algorithm, provided by `scipy.optimize` for the optimization. All you need to know about this optimizer is that it is gradient-based. Otherwise, you can treat it as a black box. Yet, it's also worth a closer look if you are interested. To install scipy, use: **conda install -c conda-forge scikit-learn**\n",
    "\n",
    "We load and visualize the data again. Note we change the label for class 0 to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "\n",
    "## load data\n",
    "train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True))\n",
    "train_samples = train_data[\"samples\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM\n",
    "train_labels[train_labels == 0] = -1\n",
    "\n",
    "test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "test_samples = test_data[\"samples\"]\n",
    "test_labels = test_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM\n",
    "test_labels[test_labels == 0] = -1\n",
    "\n",
    "## plot data\n",
    "plt.figure(figsize = (12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.grid(\"on\")\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.title(\"Train Data\")\n",
    "plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.grid(\"on\")\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.title(\"Test Data\")\n",
    "plt.scatter(x=test_samples[test_labels == -1, 0], y=test_samples[test_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=test_samples[test_labels == 1, 0], y=test_samples[test_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we use cubic features $\\phi(\\boldsymbol x)$. Let us define a function to compute those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cubic_feature_fn(samples: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: Batch of 2D data vectors [x, y] [N x dim]\n",
    "    :return cubic features: [x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, 1]\n",
    "    \"\"\"\n",
    "    x = samples[..., 0]\n",
    "    y = samples[..., 1]\n",
    "    return np.stack([x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, np.ones(x.shape[0])], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 4.1) Hinge Loss Objective (2 Points)\n",
    "\n",
    "We will implement the hinge loss objective in this exercise. Its given by\n",
    " $$\\mathcal{L}_{\\boldsymbol{X}, \\boldsymbol{y}}(\\boldsymbol w) = \\parallel \\boldsymbol{w} \\parallel^2 + C \\sum_i^N  \\max \\left( 0, 1 - y_i \\boldsymbol{w}^T \\phi(\\boldsymbol{x}_i) \\right),$$\n",
    " where $\\boldsymbol{w}$ are our model parameters, $\\phi(\\boldsymbol{x})$ are our cubic features and the $y_i \\in \\lbrace{-1, 1\\rbrace}$ are the class labels.\n",
    "\n",
    " The following function implements the hinge loss. Fill in the missing code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> float:\n",
    "    \"\"\"\n",
    "    objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape:[N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: factor to weigh the violation of margin with (C in slides)\n",
    "    :returns svm (hinge) objective (shape: scalar)\n",
    "    \"\"\"\n",
    "    ### TODO ###############################\n",
    "    \n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2) Hinge Loss Gradient (3 Points)\n",
    "Derive and implement the gradient for the hinge loss objective stated above. For all non-differentiable points in the loss curves, you can use any valid subgradient.\n",
    "\n",
    "**NOTE**: The derivation is explicitly part of this exercise, so state it in the file, not just implement it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def d_objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape: [N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: factor to weigh the violation of margin with (C in slides)\n",
    "    :returns gradient of svm objective (shape: [feature_dim])\n",
    "    \"\"\"\n",
    "    ### TODO ###############################\n",
    "    \n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "Finally, we can tie everything together and get our maximum margin classifier. Here we are using the L-BFGS-B optimizer provided by Scipy. With $C=1000$ you should end up at a training accuracy of 1 and test accuracy of 0.99, but feel free to play around with $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_fn = cubic_feature_fn\n",
    "C = 1000.0\n",
    "\n",
    "## optimization (for details cf. https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html)\n",
    "train_features = feature_fn(train_samples)\n",
    "res = opt.minimize(\n",
    "    # pass objective\n",
    "    fun=lambda w: objective_svm(w, train_features, train_labels, C),\n",
    "    # pass initial point\n",
    "    x0=np.ones(train_features.shape[-1]),\n",
    "    # pass function to evaluate gradient (in scipy.opt \"jac\" for jacobian)\n",
    "    jac=lambda w: d_objective_svm(w, train_features, train_labels, C),\n",
    "    # specify method to use\n",
    "    method=\"l-bfgs-b\"\n",
    ")\n",
    "print(res)\n",
    "w_svm = res.x\n",
    "\n",
    "## evaluation\n",
    "test_predictions = feature_fn(test_samples) @ w_svm\n",
    "train_predictions = feature_fn(train_samples) @ w_svm\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0] = -1\n",
    "print(\"Train Accuracy: \", np.count_nonzero(predicted_train_labels == train_labels) / len(train_labels))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0] = -1\n",
    "print(\"Test Accuracy: \", np.count_nonzero(predicted_test_labels == test_labels) / len(test_labels))\n",
    "\n",
    "## plot train, contour, decision boundary and margins\n",
    "plt.figure()\n",
    "plt.title(\"Max Margin Solution (Predictions clipped at(-5, 5))\")\n",
    "x_plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "y_plt_range = np.arange(-1, 1.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(x_plt_range, y_plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]\n",
    "\n",
    "pred_grid = np.reshape(feature_fn(flat_plt_grid) @ w_svm, plt_grid_shape)\n",
    "pred_grid = np.clip(pred_grid, -5, 5)\n",
    "#plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1.0, 0.0, 1.0], colors=[\"blue\", \"black\", \"orange\"])\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "            linestyles=('-',), linewidths=(2,))\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "s0 =plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "s1 =plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}